{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing COCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-wlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, w, l, p): \n",
    "        self.number = number\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.p = p\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.number : {\"w\" : self.w[:10], \"l\" : self.l[:10], \"p\" : self.p[:10]}}) + \"/n\"\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles):\n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wlp(line): \n",
    "    temp = line.split('\\t')\n",
    "    if len(temp) != 4:\n",
    "        return None\n",
    "    # return w, l, p\n",
    "    return tuple(temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(text): \n",
    "    lines = text.split(\"\\n\")\n",
    "    number = None\n",
    "    for line in lines: \n",
    "        numbers = re.findall(r'\\d+', line)\n",
    "        if len(numbers) > 0: \n",
    "            number = int(numbers[0])\n",
    "            break\n",
    "    \n",
    "    if number == None: \n",
    "        return None\n",
    "    \n",
    "    w = []\n",
    "    l = []\n",
    "    p = []\n",
    "    for line in lines: \n",
    "        args = get_wlp(line)\n",
    "        if args == None: \n",
    "            continue\n",
    "        w.append(args[0])\n",
    "        l.append(args[1])\n",
    "        p.append(args[2])\n",
    "    \n",
    "    return Article(number, w, l, p)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    pattern = r'\\d+\\t@@\\d+\\t\\t'\n",
    "    article_texts = re.split(pattern, file_text)\n",
    "    \n",
    "    articles = []\n",
    "    for text in article_texts: \n",
    "        article = parse_article(text)\n",
    "        if article == None: \n",
    "            continue\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None    \n",
    "    return File(filename, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlp_directory = \"/home/divya/Desktop/coca-samples-wlp (1)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 8\n"
     ]
    }
   ],
   "source": [
    "files = get_files(wlp_directory)\n",
    "print(f\"Number of files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wlp_spok.txt': [\"{17141: {'w': ['ERIC', '@!BURNS', ',', 'FOX', 'NEWS', 'HOST', ':', 'On', 'this', 'week'], 'l': ['eric', '', ',', 'fox', 'news', 'host', ':', 'on', 'this', 'week'], 'p': ['np1', 'zzq', 'y', 'np1_nn1', 'nn1', 'nn1_vv0', 'y', 'ii', 'dd1', 'nnt1']}}/n\", \"{21741: {'w': ['qwq', '@', '!', 'DOUGLAS-FORD-ARSO', ':', 'I', 'set', 'the', 'fire', 'at'], 'l': ['', '', '!', '', ':', 'i', 'set', 'the', 'fire', 'at'], 'p': ['xx', 'ii', 'y', 'np1', 'y', 'ppis1', 'vv0_vvd', 'at', 'nn1', 'ii']}}/n\", \"{207541: {'w': ['(', 'BEGIN', 'VIDEOTAPE', ')', 'HOWARD', 'KURTZ', ',', 'HOST', '(', 'voice-over'], 'l': ['(', 'begin', 'videotape', '', 'howard', 'kurtz', ',', 'host', '(', 'voice-over'], 'p': ['y', 'vv0', 'np1_nn1', 'np1', 'np1', 'np1', 'y', 'nn1_vv0', 'y', 'np']}}/n\"]}\n"
     ]
    }
   ],
   "source": [
    "wlp_spok_file = [file for file in files if \"wlp_spok\" in file.filename][0]\n",
    "wlp_tvm_file = [file for file in files if \"wlp_tvm\" in file.filename][0]\n",
    "print(wlp_spok_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(file1, file2): \n",
    "    X, Y = [], []\n",
    "    for a in file1.articles: \n",
    "        X.append(a.l)\n",
    "        Y.append(0)\n",
    "    for a in file2.articles: \n",
    "        X.append(a.l)\n",
    "        Y.append(1)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(wlp_spok_file, wlp_tvm_file)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_feature(tokens): \n",
    "    feats = {}\n",
    "    for token in tokens: \n",
    "        feats[token] = feats.get(token, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.949\n"
     ]
    }
   ],
   "source": [
    "features = [count_feature]\n",
    "clf, vocab = pipeline(trainX, testX, trainY, testY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.207\t:\n",
      "-0.139\tknow\n",
      "-0.137\tbut\n",
      "-0.134\tyou\n",
      "-0.133\ta\n",
      "-0.129\tso\n",
      "-0.115\twoman\n",
      "-0.113\tin\n",
      "-0.110\tthis\n",
      "-0.089\tand\n",
      "\n",
      "0.228\t!\n",
      "0.208\t?\n",
      "0.194\tme\n",
      "0.190\ti\n",
      "0.174\t\"\n",
      "0.158\tget\n",
      "0.137\tsex\n",
      "0.121\t...\n",
      "0.114\tn't\n",
      "0.094\ttrafficking\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

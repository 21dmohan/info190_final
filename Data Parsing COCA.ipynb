{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing COCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-wlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, w, l, p, s=[]): \n",
    "        self.number = number\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.p = p\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.number : {\"w\" : self.w[:10], \"l\" : self.l[:10], \"p\" : self.p[:10]}})\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles):\n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(): \n",
    "        return f\"\\{ {self.filename} : {[str(a) for a in self.articles[:5]]} ... \\}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wlp(line): \n",
    "    temp = line.split('\\t')\n",
    "    if len(temp) != 4:\n",
    "        return None\n",
    "    # return w, l, p\n",
    "    return tuple(temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(text): \n",
    "    lines = text.split(\"\\n\")\n",
    "    number = None\n",
    "    for line in lines: \n",
    "        numbers = re.findall(r'\\d+', line)\n",
    "        if len(numbers) > 0: \n",
    "            number = int(numbers[0])\n",
    "            break\n",
    "    \n",
    "    if number == None: \n",
    "        return None\n",
    "    \n",
    "    w = []\n",
    "    l = []\n",
    "    p = []\n",
    "    for line in lines: \n",
    "        args = get_wlp(line)\n",
    "        if args == None: \n",
    "            continue\n",
    "        w.append(args[0])\n",
    "        l.append(args[1])\n",
    "        p.append(args[2])\n",
    "    \n",
    "    return Article(number, w, l, p)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    pattern = r'\\d+\\t@@\\d+\\t\\t'\n",
    "    article_texts = re.split(pattern, file_text)\n",
    "    \n",
    "    articles = []\n",
    "    for text in article_texts: \n",
    "        article = parse_article(text)\n",
    "        if article == None: \n",
    "            continue\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None    \n",
    "    return File(filename, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Files and Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlp_directory = \"/home/divya/Desktop/coca-samples-wlp (1)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 8\n"
     ]
    }
   ],
   "source": [
    "files = get_files(wlp_directory)\n",
    "print(f\"Number of files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: wlp_mag.txt. Number of articles: 948.\n"
     ]
    }
   ],
   "source": [
    "file = files[0]\n",
    "print(f\"Filename: {file.filename}. Number of articles: {len(file.articles)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article #: 2000341. Number of words: 507.\n",
      "Sample of words: ['Al', 'Sobotka', ',', 'has', 'been', 'driving', 'a', 'Zamboni', 'for', 'the'].\n",
      "Sample of l: ['al', 'sobotka', ',', 'have', 'be', 'drive', 'a', 'zamboni', 'for', 'the'].\n",
      "Sample of p: ['np1', 'np1', 'y', 'vhz', 'vbn', 'vvg', 'at1', 'nn1_jj', 'if', 'at'].\n"
     ]
    }
   ],
   "source": [
    "article = file.articles[0]\n",
    "print(f\"Article #: {article.number}. Number of words: {len(article.w)}.\")\n",
    "print(f\"Sample of words: {article.w[:10]}.\")\n",
    "print(f\"Sample of l: {article.l[:10]}.\")\n",
    "print(f\"Sample of p: {article.p[:10]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: This section is incomplete and may take a long time to run. I suggest not running these cells :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/divya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, sentence_tokens, sentence_tokens_wo_sw=[]): \n",
    "        self.number = number\n",
    "        self.sentence_tokens = sentence_tokens\n",
    "        self.sentence_tokens_wo_sw = sentence_tokens_wo_sw\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles): \n",
    "        self.filename = filename\n",
    "        self.articles = articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence_text): \n",
    "    text_tokens = word_tokenize(sentence_text)\n",
    "    if len(text_tokens) == 0:\n",
    "        return None\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    return (text_tokens, tokens_without_sw)\n",
    "\n",
    "def get_sentences(article_text): \n",
    "    pattern = r\" [\\.|\\?|\\!] \"\n",
    "    sentences = re.split(pattern, article_text)\n",
    "    return sentences\n",
    "\n",
    "def get_article_texts(file_text): \n",
    "    pattern = r'\\n@@\\d+ '\n",
    "    articles = re.split(pattern, file_text)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(file_text): \n",
    "    article_texts = get_article_texts(file_text)\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "\n",
    "        sentence_texts = get_sentences(article_text)\n",
    "\n",
    "        sentence_tokens = []\n",
    "        sentence_tokens_wo_sw = []\n",
    "        for sentence_text in sentence_texts: \n",
    "            rv = get_tokens(sentence_text)\n",
    "            if rv == None:\n",
    "                continue\n",
    "            (text_tokens, tokens_without_sw) = rv\n",
    "            sentence_tokens.append(text_tokens)\n",
    "            sentence_tokens_wo_sw.append(tokens_without_sw)\n",
    "\n",
    "        if len(sentence_tokens) == 0: \n",
    "            continue\n",
    "        article = Article(None, sentence_tokens, sentence_tokens_wo_sw)\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = File(filename, articles)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of File and Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_directory = \"/home/divya/Desktop/coca-samples-text/\"\n",
    "text_spok = \"text_spok.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "files = get_files(text_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file = parse_filename(text_directory, text_spok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

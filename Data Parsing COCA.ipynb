{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing COCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-wlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, w, l, p, s=[]): \n",
    "        self.number = number\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.p = p\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.number : {\"w\" : self.w[:10], \"l\" : self.l[:10], \"p\" : self.p[:10]}}) + \"\\n\"\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles):\n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wlp(line): \n",
    "    temp = line.split('\\t')\n",
    "    if len(temp) != 4:\n",
    "        return None\n",
    "    # return w, l, p\n",
    "    return tuple(temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(text): \n",
    "    lines = text.split(\"\\n\")\n",
    "    number = None\n",
    "    for line in lines: \n",
    "        numbers = re.findall(r'\\d+', line)\n",
    "        if len(numbers) > 0: \n",
    "            number = int(numbers[0])\n",
    "            break\n",
    "    \n",
    "    if number == None: \n",
    "        return None\n",
    "    \n",
    "    w = []\n",
    "    l = []\n",
    "    p = []\n",
    "    for line in lines: \n",
    "        args = get_wlp(line)\n",
    "        if args == None: \n",
    "            continue\n",
    "        w.append(args[0])\n",
    "        l.append(args[1])\n",
    "        p.append(args[2])\n",
    "    \n",
    "    return Article(number, w, l, p)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    pattern = r'\\d+\\t@@\\d+\\t\\t'\n",
    "    article_texts = re.split(pattern, file_text)\n",
    "    \n",
    "    articles = []\n",
    "    for text in article_texts: \n",
    "        article = parse_article(text)\n",
    "        if article == None: \n",
    "            continue\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None    \n",
    "    return File(filename, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Files and Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlp_directory = \"/home/divya/Desktop/coca-samples-wlp (1)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 8\n"
     ]
    }
   ],
   "source": [
    "files = get_files(wlp_directory)\n",
    "print(f\"Number of files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: wlp_mag.txt. Number of articles: 948.\n"
     ]
    }
   ],
   "source": [
    "file = files[0]\n",
    "print(f\"Filename: {file.filename}. Number of articles: {len(file.articles)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article #: 2000341. Number of words: 507.\n",
      "Sample of words: ['Al', 'Sobotka', ',', 'has', 'been', 'driving', 'a', 'Zamboni', 'for', 'the'].\n",
      "Sample of l: ['al', 'sobotka', ',', 'have', 'be', 'drive', 'a', 'zamboni', 'for', 'the'].\n",
      "Sample of p: ['np1', 'np1', 'y', 'vhz', 'vbn', 'vvg', 'at1', 'nn1_jj', 'if', 'at'].\n"
     ]
    }
   ],
   "source": [
    "article = file.articles[0]\n",
    "print(f\"Article #: {article.number}. Number of words: {len(article.w)}.\")\n",
    "print(f\"Sample of words: {article.w[:10]}.\")\n",
    "print(f\"Sample of l: {article.l[:10]}.\")\n",
    "print(f\"Sample of p: {article.p[:10]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for Midterm Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: wlp_mag.txt\n",
      "Number of articles: 948.\n",
      "Average Number of Words per Article: 1652.0611814345991\n",
      "----------\n",
      "Filename: wlp_spok.txt\n",
      "Number of articles: 262.\n",
      "Average Number of Words per Article: 4428.412213740458\n",
      "----------\n",
      "Filename: wlp_tvm.txt\n",
      "Number of articles: 226.\n",
      "Average Number of Words per Article: 6935.115044247787\n",
      "----------\n",
      "Filename: wlp_fic.txt\n",
      "Number of articles: 274.\n",
      "Average Number of Words per Article: 5130.029197080292\n",
      "----------\n",
      "Filename: wlp_acad.txt\n",
      "Number of articles: 265.\n",
      "Average Number of Words per Article: 5355.603773584906\n",
      "----------\n",
      "Filename: wlp_news.txt\n",
      "Number of articles: 871.\n",
      "Average Number of Words per Article: 1594.5832376578646\n",
      "----------\n",
      "Filename: wlp_blog.txt\n",
      "Number of articles: 990.\n",
      "Average Number of Words per Article: 1601.1151515151514\n",
      "----------\n",
      "Filename: wlp_web.txt\n",
      "Number of articles: 892.\n",
      "Average Number of Words per Article: 1594.9159192825111\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for file in files: \n",
    "    print(f\"Filename: {file.filename}\")\n",
    "    print(f\"Number of articles: {len(file.articles)}.\")\n",
    "    avg_article_len = sum([len(a.w) for a in file.articles]) / len(file.articles)\n",
    "    print(f\"Average Number of Words per Article: {avg_article_len}\")\n",
    "    print(\"----------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: This section is incomplete and may take a long time to run. I suggest not running these cells :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/divya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, text, sentence_texts, sentence_tokens=[], sentence_tokens_wo_sw=[]): \n",
    "        self.number = number\n",
    "        self.text = text\n",
    "        self.sentence_texts = sentence_texts\n",
    "        self.sentence_tokens = sentence_tokens\n",
    "        self.sentence_tokens_wo_sw = sentence_tokens_wo_sw\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({self.number : {\"text\" : self.text[:50], \"sentences\" : [s[:10] for s in self.sentence_texts[:3]]}})\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles): \n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(file_text): \n",
    "    article_texts = file_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "        if len(article_text) == 0: \n",
    "            continue\n",
    "        \n",
    "        pattern = r'@@\\d+ '\n",
    "        rv = re.findall(pattern, article_text[:20])\n",
    "        if len(rv) == 0: \n",
    "            continue\n",
    "        article_number = int(rv[0][2:-1])\n",
    "\n",
    "        pattern = r\" [\\.|\\?|\\!] \"\n",
    "        sentence_texts = re.split(pattern, article_text)[1:]\n",
    "        if len(sentence_texts) == 0: \n",
    "            continue\n",
    "        \n",
    "        # article = Article(article_number, article_text, sentence_texts, sentence_tokens, sentence_tokens_wo_sw)\n",
    "        article = Article(article_number, article_text, sentence_texts)\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = File(filename, articles)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of File and Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_directory = \"/home/divya/Desktop/coca-samples-text/\"\n",
    "text_spok = \"text_spok.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 165 ms, total: 1.84 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files = get_files(text_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_news.txt': [\"{3000041: {'text': '@@3000041 <p> Basketball United States Wins Bronze', 'sentences': ['Kenny Ande', 'Yugoslavia', 'This was t']}}\", \"{3000341: {'text': '@@3000341 <p> In the airy sitting room of the ranc', 'sentences': ['In the fra', '<p> Nelson', 'He is the ']}}\", \"{3000641: {'text': '@@3000641 <p> It was just a normal summer day when', 'sentences': ['<p> Just o', 'Not the hi', 'Completely']}}\"]}\n"
     ]
    }
   ],
   "source": [
    "file = files[0]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: text_news.txt\n",
      "Number of articles: 871\n",
      "Average number of sentences per article: 69.59931113662456\n",
      "----------\n",
      "Filename: text_fic.txt\n",
      "Number of articles: 274\n",
      "Average number of sentences per article: 325.3430656934307\n",
      "----------\n",
      "Filename: text_web.txt\n",
      "Number of articles: 856\n",
      "Average number of sentences per article: 71.50934579439253\n",
      "----------\n",
      "Filename: text_spok.txt\n",
      "Number of articles: 263\n",
      "Average number of sentences per article: 245.08365019011407\n",
      "----------\n",
      "Filename: text_tvm.txt\n",
      "Number of articles: 233\n",
      "Average number of sentences per article: 783.2145922746781\n",
      "----------\n",
      "Filename: text_blog.txt\n",
      "Number of articles: 982\n",
      "Average number of sentences per article: 69.70875763747455\n",
      "----------\n",
      "Filename: text_acad.txt\n",
      "Number of articles: 265\n",
      "Average number of sentences per article: 190.25283018867924\n",
      "----------\n",
      "Filename: text_mag.txt\n",
      "Number of articles: 947\n",
      "Average number of sentences per article: 69.23125659978881\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for file in files: \n",
    "    print(f\"Filename: {file.filename}\")\n",
    "    print(f\"Number of articles: {len(file.articles)}\")\n",
    "    avg_article_sent = sum([len(a.sentence_texts) for a in file.articles]) / len(file.articles)\n",
    "    print(f\"Average number of sentences per article: {avg_article_sent}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Lexicon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/divya/Desktop/coca-samples-lexicon (1)/lexicon.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(file_name, \"r\", encoding=\"ISO-8859-1\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "lines = text.split(\"\\n\")\n",
    "lexicon = []\n",
    "for line in lines: \n",
    "    rv = line.split(\"\\t\")\n",
    "    if len(rv) == 4: \n",
    "        (_, w, l, p) = rv\n",
    "        lexicon.append((w, l, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(w): \n",
    "    return [t for t in lexicon if t[0] == w]\n",
    "\n",
    "def get_lexicon(l): \n",
    "    return [t for t in lexicon if t[1] == l]\n",
    "\n",
    "def get_pos(p): \n",
    "    return [t for t in lexicon if t[2] == p]\n",
    "\n",
    "def get_all_words(): \n",
    "    return list(set([t[0] for t in lexicon]))\n",
    "\n",
    "def get_all_lexicons(): \n",
    "    return list(set([t[1] for t in lexicon]))\n",
    "\n",
    "def get_all_POSs(): \n",
    "    return list(set([t[2] for t in lexicon]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 7104688\n",
      "Number of Lexicons: 2032411\n",
      "Number of Parts-Of-Speech: 12090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Words: {len(get_all_words())}\")\n",
    "print(f\"Number of Lexicons: {len(get_all_lexicons())}\")\n",
    "print(f\"Number of Parts-Of-Speech: {len(get_all_POSs())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing COCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/divya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, text, sentence_texts, sentence_tokens=[], sentence_tokens_wo_sw=[]): \n",
    "        self.number = number\n",
    "        self.text = text\n",
    "        self.sentence_texts = sentence_texts\n",
    "        self.sentence_tokens = sentence_tokens\n",
    "        self.sentence_tokens_wo_sw = sentence_tokens_wo_sw\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({self.number : {\"text\" : self.text[:50], \"sentences\" : [s[:10] for s in self.sentence_texts[:3]]}})\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles): \n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(file_text): \n",
    "    article_texts = file_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "        if len(article_text) == 0: \n",
    "            continue\n",
    "        \n",
    "        pattern = r'@@\\d+ '\n",
    "        rv = re.findall(pattern, article_text[:20])\n",
    "        if len(rv) == 0: \n",
    "            continue\n",
    "        article_number = int(rv[0][2:-1])\n",
    "\n",
    "        pattern = r\" [\\.|\\?|\\!] \"\n",
    "        sentence_texts = re.split(pattern, article_text)[1:]\n",
    "        if len(sentence_texts) == 0: \n",
    "            continue\n",
    "        \n",
    "        # article = Article(article_number, article_text, sentence_texts, sentence_tokens, sentence_tokens_wo_sw)\n",
    "        article = Article(article_number, article_text, sentence_texts)\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = File(filename, articles)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_directory = \"data/coca-samples-text/\"\n",
    "text_files = get_files(text_directory)\n",
    "text_spok_file = [file for file in text_files if \"text_spok\" in file.filename][0]\n",
    "text_tvm_file = [file for file in text_files if \"text_tvm\" in file.filename][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files: 8\n",
      "{'text_spok.txt': [\"{17141: {'text': '@@17141 ERIC @!BURNS , FOX NEWS HOST : On this wee', 'sentences': ['The media ', 'How did th', 'How did au']}}\", \"{21741: {'text': '@@21741 qwq @ ! DOUGLAS-FORD-ARSO : I set the fire', 'sentences': ['DOUGLAS-FO', 'I burned i', 'I cant say']}}\", \"{207541: {'text': '@@207541 ( BEGIN VIDEOTAPE ) HOWARD KURTZ , HOST (', 'sentences': ['Were ABC a', 'Should Geo', 'With Iraq ']}}\"]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Files: {len(text_files)}\")\n",
    "print(text_spok_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(file1, file2): \n",
    "    X, Y = [], []\n",
    "    for a in file1.articles: \n",
    "        X.append(a.sentence_texts)\n",
    "        Y.append(0)\n",
    "    for a in file2.articles: \n",
    "        X.append(a.sentence_texts)\n",
    "        Y.append(1)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(text_spok_file, text_tvm_file)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['largely', 'generally', 'often', 'rarely', 'sometimes', 'frequently', 'occasionally', 'seldom', 'usually', 'most', 'several', 'some', 'almost', 'practically', 'apparently', 'virtually', 'basically', 'approximately', 'roughly', 'somewhat', 'somehow', 'partially', 'actually', 'like', 'something', 'someone', 'somebody', 'somewhere', 'think', 'thinks', 'thought', 'believe', 'believed', 'believes', 'consider', 'considers', 'considered', 'assume', 'assumes', 'assumed', 'understand', 'understands', 'understood', 'find', 'found', 'finds', 'appear', 'appears', 'appeared', 'seem', 'seems', 'seemed', 'suppose', 'supposes', 'supposed', 'guess', 'guesses', 'guessed', 'estimate', 'estimates', 'estimated', 'speculate', 'speculates', 'speculated', 'suggest', 'suggests', 'suggested', 'may', 'could', 'should', 'might', 'surely', 'probably', 'likely', 'maybe', 'perhaps', 'unsure', 'probable', 'unlikely', 'possibly', 'possible', 'read', 'say', 'says', 'looks like', 'look like', \"don't know\", 'necessarily', 'kind of', 'much', 'bunch', 'couple', 'few', 'little', 'really', 'and all that', 'and so forth', 'et cetera', 'in my mind', 'in my opinion', 'their impression', 'my impression', 'in my understanding', 'my thinking is', 'my understanding is', 'in my view', \"if i'm understanding you correctly\", 'something or other', 'so far', 'at least', 'about', 'around', 'can', 'effectively', 'evidently', 'fairly', 'hopefully', 'in general', 'mainly', 'more or less', 'mostly', 'overall', 'presumably', 'pretty', 'quite clearly', 'quite', 'rather', 'sort of', 'supposedly', 'tend', 'appear to be', 'doubt', 'be sure', 'indicate', 'will', 'must', 'would', 'certainly', 'definitely', 'clearly', 'conceivably', 'certain', 'definite', 'clear', 'assumption', 'possibility', 'probability', 'many', 'almost never', 'improbable', 'always', 'rare', 'consistent with', 'doubtful', 'suggestive', 'diagnostic', 'inconclusive', 'apparent', 'alleged', 'allege', 'a bit', 'presumable']\n"
     ]
    }
   ],
   "source": [
    "hedging_file = open(\"data/hedging_data.txt\")\n",
    "hedging_text = hedging_file.read()\n",
    "hedging_file.close()\n",
    "hedging_lst = [h for h in hedging_text.split(\"\\n\") if (\"%\" not in h and len(h) > 2)]\n",
    "print(hedging_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedging_feature(sentence_texts): \n",
    "    feats = {}\n",
    "    for sentence in sentence_texts: \n",
    "        for hedge in hedging_lst: \n",
    "            if hedge in sentence: \n",
    "                feats[hedge] = feats.get(hedge, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "features = [hedging_feature]\n",
    "clf, vocab = pipeline(trainX, testX, trainY, testY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.702\tseveral\n",
      "-5.650\tallege\n",
      "-5.650\talleged\n",
      "-5.425\tso far\n",
      "-4.777\tpossibility\n",
      "-4.466\tsort of\n",
      "-4.343\tsomewhat\n",
      "-3.899\tnecessarily\n",
      "-3.813\testimates\n",
      "-3.608\tcouple\n",
      "\n",
      "5.305\tlook like\n",
      "4.075\tpractically\n",
      "4.073\tconsistent with\n",
      "4.022\tfinds\n",
      "3.766\tlooks like\n",
      "3.014\ta bit\n",
      "2.920\tassumption\n",
      "2.832\trare\n",
      "2.744\tappeared\n",
      "2.696\tfew\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

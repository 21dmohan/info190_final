{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing COCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coca-samples-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/divya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article: \n",
    "    def __init__(self, number, text, sentence_texts, sentence_tokens=[], sentence_tokens_wo_sw=[]): \n",
    "        self.number = number\n",
    "        self.text = text\n",
    "        self.sentence_texts = sentence_texts\n",
    "        self.sentence_tokens = sentence_tokens\n",
    "        self.sentence_tokens_wo_sw = sentence_tokens_wo_sw\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({self.number : {\"text\" : self.text[:50], \"sentences\" : [s[:10] for s in self.sentence_texts[:3]]}})\n",
    "\n",
    "class File: \n",
    "    def __init__(self, filename, articles): \n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(file_text): \n",
    "    article_texts = file_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "        if len(article_text) == 0: \n",
    "            continue\n",
    "        \n",
    "        pattern = r'@@\\d+ '\n",
    "        rv = re.findall(pattern, article_text[:20])\n",
    "        if len(rv) == 0: \n",
    "            continue\n",
    "        article_number = int(rv[0][2:-1])\n",
    "\n",
    "        pattern = r\" [\\.|\\?|\\!] \"\n",
    "        sentence_texts = re.split(pattern, article_text)[1:]\n",
    "        if len(sentence_texts) == 0: \n",
    "            continue\n",
    "        \n",
    "        # article = Article(article_number, article_text, sentence_texts, sentence_tokens, sentence_tokens_wo_sw)\n",
    "        article = Article(article_number, article_text, sentence_texts)\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = File(filename, articles)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_directory = \"/home/divya/Desktop/coca-samples-text/\"\n",
    "text_files = get_files(text_directory)\n",
    "text_spok_file = [file for file in text_files if \"text_spok\" in file.filename][0]\n",
    "text_tvm_file = [file for file in text_files if \"text_tvm\" in file.filename][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files: 8\n",
      "{'text_spok.txt': [\"{17141: {'text': '@@17141 ERIC @!BURNS , FOX NEWS HOST : On this wee', 'sentences': ['The media ', 'How did th', 'How did au']}}\", \"{21741: {'text': '@@21741 qwq @ ! DOUGLAS-FORD-ARSO : I set the fire', 'sentences': ['DOUGLAS-FO', 'I burned i', 'I cant say']}}\", \"{207541: {'text': '@@207541 ( BEGIN VIDEOTAPE ) HOWARD KURTZ , HOST (', 'sentences': ['Were ABC a', 'Should Geo', 'With Iraq ']}}\"]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Files: {len(text_files)}\")\n",
    "print(text_spok_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(file1, file2): \n",
    "    X, Y = [], []\n",
    "    for a in file1.articles: \n",
    "        X.append(a.sentence_texts)\n",
    "        Y.append(0)\n",
    "    for a in file2.articles: \n",
    "        X.append(a.sentence_texts)\n",
    "        Y.append(1)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(text_spok_file, text_tvm_file)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_feature(sentence_texts): \n",
    "    feats = {}\n",
    "    for sentence in sentence_texts: \n",
    "        num_tokens = len(sentence.split(\" \"))\n",
    "        feat_name = f\"sentence_length_{num_tokens}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.930\n"
     ]
    }
   ],
   "source": [
    "features = [count_feature]\n",
    "clf, vocab = pipeline(trainX, testX, trainY, testY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.545\tsentence_length_24\n",
      "-1.262\tsentence_length_21\n",
      "-1.025\tsentence_length_19\n",
      "-0.948\tsentence_length_20\n",
      "-0.906\tsentence_length_10\n",
      "-0.902\tsentence_length_47\n",
      "-0.823\tsentence_length_37\n",
      "-0.768\tsentence_length_35\n",
      "-0.752\tsentence_length_42\n",
      "-0.688\tsentence_length_26\n",
      "\n",
      "2.333\tsentence_length_125\n",
      "2.333\tsentence_length_93\n",
      "2.269\tsentence_length_49\n",
      "1.212\tsentence_length_9\n",
      "0.953\tsentence_length_1\n",
      "0.836\tsentence_length_77\n",
      "0.767\tsentence_length_56\n",
      "0.751\tsentence_length_23\n",
      "0.734\tsentence_length_111\n",
      "0.728\tsentence_length_75\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coca-samples-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles_text(file_text): \n",
    "    article_texts = file_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "        if len(article_text) == 0: \n",
    "            continue\n",
    "        \n",
    "        pattern = r'@@\\d+ '\n",
    "        rv = re.findall(pattern, article_text[:20])\n",
    "        if len(rv) == 0: \n",
    "            continue\n",
    "        article_number = int(rv[0][2:-1])\n",
    "        print(f\"article_number: {article_number}\")\n",
    "        article_doc = nlp(article_text)\n",
    "        article = {\"number\" : article_number, \"doc\" : article_doc}\n",
    "        \n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename_text(directory, filename): \n",
    "    print(f\"filename: {filename}\")\n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles_text(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = {\"filename\" : filename, \"articles\" : articles}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_text(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename_text(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: text_spok.txt\n",
      "article_number: 17141\n",
      "article_number: 21741\n",
      "article_number: 207541\n",
      "article_number: 207641\n",
      "article_number: 220641\n",
      "article_number: 220741\n",
      "article_number: 221141\n",
      "article_number: 221241\n",
      "article_number: 221341\n",
      "article_number: 221441\n",
      "article_number: 221541\n",
      "article_number: 221641\n",
      "article_number: 221741\n",
      "article_number: 222141\n",
      "article_number: 222241\n",
      "article_number: 222341\n",
      "article_number: 222441\n",
      "article_number: 222541\n",
      "article_number: 222741\n",
      "article_number: 222841\n",
      "article_number: 222941\n",
      "article_number: 223041\n",
      "article_number: 223141\n",
      "article_number: 223241\n",
      "article_number: 223341\n",
      "article_number: 223441\n",
      "article_number: 223541\n",
      "article_number: 223941\n",
      "article_number: 224241\n",
      "article_number: 224341\n",
      "article_number: 224541\n",
      "article_number: 224641\n",
      "article_number: 224741\n",
      "article_number: 224841\n",
      "article_number: 224941\n",
      "article_number: 225041\n",
      "article_number: 225141\n",
      "article_number: 225241\n",
      "article_number: 225341\n",
      "article_number: 225441\n",
      "article_number: 225541\n",
      "article_number: 225641\n",
      "article_number: 225741\n",
      "article_number: 226741\n",
      "article_number: 227641\n",
      "article_number: 228041\n",
      "article_number: 228141\n",
      "article_number: 228241\n",
      "article_number: 228341\n",
      "article_number: 228441\n",
      "article_number: 228541\n",
      "article_number: 228641\n",
      "article_number: 228741\n",
      "article_number: 228841\n",
      "article_number: 228941\n",
      "article_number: 229641\n",
      "article_number: 229741\n",
      "article_number: 230641\n",
      "article_number: 230941\n",
      "article_number: 231041\n",
      "article_number: 231141\n",
      "article_number: 231241\n",
      "article_number: 231441\n",
      "article_number: 231541\n",
      "article_number: 231641\n",
      "article_number: 231741\n",
      "article_number: 231841\n",
      "article_number: 231941\n",
      "article_number: 232041\n",
      "article_number: 232141\n",
      "article_number: 232241\n",
      "article_number: 232441\n",
      "article_number: 232541\n",
      "article_number: 232641\n",
      "article_number: 232741\n",
      "article_number: 232841\n",
      "article_number: 232941\n",
      "article_number: 233041\n",
      "article_number: 233141\n",
      "article_number: 233241\n",
      "article_number: 233641\n",
      "article_number: 234241\n",
      "article_number: 234341\n",
      "article_number: 234441\n",
      "article_number: 234541\n",
      "article_number: 234641\n",
      "article_number: 234741\n",
      "article_number: 234841\n",
      "article_number: 234941\n",
      "article_number: 235041\n",
      "article_number: 235141\n",
      "article_number: 235241\n",
      "article_number: 235341\n",
      "article_number: 235541\n",
      "article_number: 235641\n",
      "article_number: 235741\n",
      "article_number: 235841\n",
      "article_number: 235941\n",
      "article_number: 236041\n",
      "article_number: 236641\n",
      "article_number: 236841\n",
      "article_number: 236941\n",
      "article_number: 237041\n",
      "article_number: 237141\n",
      "article_number: 237241\n",
      "article_number: 237341\n",
      "article_number: 237441\n",
      "article_number: 237541\n",
      "article_number: 237641\n",
      "article_number: 237741\n",
      "article_number: 237941\n",
      "article_number: 238041\n",
      "article_number: 238241\n",
      "article_number: 238341\n",
      "article_number: 238441\n",
      "article_number: 238541\n",
      "article_number: 238841\n",
      "article_number: 238941\n",
      "article_number: 239041\n",
      "article_number: 239241\n",
      "article_number: 239441\n",
      "article_number: 239641\n",
      "article_number: 241841\n",
      "article_number: 241941\n",
      "article_number: 242041\n",
      "article_number: 242141\n",
      "article_number: 242441\n",
      "article_number: 244741\n",
      "article_number: 244841\n",
      "article_number: 244941\n",
      "article_number: 246241\n",
      "article_number: 246341\n",
      "article_number: 246441\n",
      "article_number: 246541\n",
      "article_number: 246641\n",
      "article_number: 246741\n",
      "article_number: 247241\n",
      "article_number: 247341\n",
      "article_number: 247541\n",
      "article_number: 247641\n",
      "article_number: 247741\n",
      "article_number: 247841\n",
      "article_number: 247941\n",
      "article_number: 248041\n",
      "article_number: 248141\n",
      "article_number: 248641\n",
      "article_number: 248741\n",
      "article_number: 248841\n",
      "article_number: 248941\n",
      "article_number: 249241\n",
      "article_number: 249341\n",
      "article_number: 253841\n",
      "article_number: 253941\n",
      "article_number: 254241\n",
      "article_number: 4022141\n",
      "article_number: 4022241\n",
      "article_number: 4023741\n",
      "article_number: 4025441\n",
      "article_number: 4026341\n",
      "article_number: 4026541\n",
      "article_number: 4026941\n",
      "article_number: 4027141\n",
      "article_number: 4027841\n",
      "article_number: 4028241\n",
      "article_number: 4030041\n",
      "article_number: 4030241\n",
      "article_number: 4030941\n",
      "article_number: 4031041\n",
      "article_number: 4031141\n",
      "article_number: 4031241\n",
      "article_number: 4031441\n",
      "article_number: 4031641\n",
      "article_number: 4032041\n",
      "article_number: 4072041\n",
      "article_number: 4072141\n",
      "article_number: 4072241\n",
      "article_number: 4072341\n",
      "article_number: 4072441\n",
      "article_number: 4072641\n",
      "article_number: 4072741\n",
      "article_number: 4072841\n",
      "article_number: 4072941\n",
      "article_number: 4073041\n",
      "article_number: 4081941\n",
      "article_number: 4082041\n",
      "article_number: 4082141\n",
      "article_number: 4082241\n",
      "article_number: 4082341\n",
      "article_number: 4082441\n",
      "article_number: 4082541\n",
      "article_number: 4082641\n",
      "article_number: 4082741\n",
      "article_number: 4090741\n",
      "article_number: 4090841\n",
      "article_number: 4102741\n",
      "article_number: 4102841\n",
      "article_number: 4102941\n",
      "article_number: 4103141\n",
      "article_number: 4103341\n",
      "article_number: 4103441\n",
      "article_number: 4103541\n",
      "article_number: 4103641\n",
      "article_number: 4103741\n",
      "article_number: 4103841\n",
      "article_number: 4103941\n",
      "article_number: 4104041\n",
      "article_number: 4104141\n",
      "article_number: 4104241\n",
      "article_number: 4122841\n",
      "article_number: 4122941\n",
      "article_number: 4123341\n",
      "article_number: 4123541\n",
      "article_number: 4124141\n",
      "article_number: 4124241\n",
      "article_number: 4124441\n",
      "article_number: 4124541\n",
      "article_number: 4124741\n",
      "article_number: 4171141\n",
      "article_number: 4171241\n",
      "article_number: 4171341\n",
      "article_number: 4171441\n",
      "article_number: 4171541\n",
      "article_number: 4171641\n",
      "article_number: 4171741\n",
      "article_number: 4171841\n",
      "article_number: 4171941\n",
      "article_number: 4172141\n",
      "article_number: 4172241\n",
      "article_number: 4172341\n",
      "article_number: 4172441\n",
      "article_number: 4172541\n",
      "article_number: 4172641\n",
      "article_number: 4172642\n",
      "article_number: 4172841\n",
      "article_number: 4172941\n",
      "article_number: 4173041\n",
      "article_number: 5000041\n",
      "article_number: 5000141\n",
      "article_number: 5000241\n",
      "article_number: 5000341\n",
      "article_number: 5000441\n",
      "article_number: 5000541\n",
      "article_number: 5000641\n",
      "article_number: 5000741\n",
      "article_number: 5000841\n",
      "article_number: 5000941\n",
      "article_number: 5001041\n",
      "article_number: 5001141\n",
      "article_number: 5001241\n",
      "article_number: 5001341\n",
      "article_number: 5001441\n",
      "article_number: 5001541\n",
      "article_number: 5001641\n",
      "article_number: 5001741\n",
      "article_number: 5001841\n",
      "article_number: 5001941\n",
      "article_number: 5002041\n",
      "article_number: 5002141\n",
      "article_number: 5002241\n",
      "article_number: 5002341\n",
      "article_number: 5002441\n",
      "article_number: 5002541\n",
      "article_number: 5002641\n",
      "filename: text_tvm.txt\n",
      "article_number: 5208241\n",
      "article_number: 5208341\n",
      "article_number: 5208441\n",
      "article_number: 5208541\n",
      "article_number: 5208741\n",
      "article_number: 5209041\n",
      "article_number: 5209141\n",
      "article_number: 5209241\n",
      "article_number: 5209341\n",
      "article_number: 5209441\n",
      "article_number: 5209541\n",
      "article_number: 5210141\n",
      "article_number: 5210241\n",
      "article_number: 5210441\n",
      "article_number: 5211041\n",
      "article_number: 5211241\n",
      "article_number: 5211441\n",
      "article_number: 5211541\n",
      "article_number: 5212041\n",
      "article_number: 5212141\n",
      "article_number: 5212241\n",
      "article_number: 5212341\n",
      "article_number: 5212841\n",
      "article_number: 5212941\n",
      "article_number: 5213041\n",
      "article_number: 5213141\n",
      "article_number: 5213241\n",
      "article_number: 5214041\n",
      "article_number: 5214141\n",
      "article_number: 5214441\n",
      "article_number: 5214541\n",
      "article_number: 5215141\n",
      "article_number: 5215241\n",
      "article_number: 5215341\n",
      "article_number: 5215441\n",
      "article_number: 5215541\n",
      "article_number: 5215641\n",
      "article_number: 5215941\n",
      "article_number: 5216041\n",
      "article_number: 5216141\n",
      "article_number: 5216241\n",
      "article_number: 5216441\n",
      "article_number: 5216941\n",
      "article_number: 5217141\n",
      "article_number: 5217441\n",
      "article_number: 5217741\n",
      "article_number: 5218041\n",
      "article_number: 5218241\n",
      "article_number: 5218341\n",
      "article_number: 5218441\n",
      "article_number: 5218541\n",
      "article_number: 5218641\n",
      "article_number: 5219041\n",
      "article_number: 5219241\n",
      "article_number: 5219341\n",
      "article_number: 5219541\n",
      "article_number: 5220041\n",
      "article_number: 5220141\n",
      "article_number: 5220441\n",
      "article_number: 5220541\n",
      "article_number: 5220641\n",
      "article_number: 5221041\n",
      "article_number: 5221141\n",
      "article_number: 5221341\n",
      "article_number: 5221441\n",
      "article_number: 5222041\n",
      "article_number: 5222141\n",
      "article_number: 5222541\n",
      "article_number: 5223241\n",
      "article_number: 5223341\n",
      "article_number: 5223441\n",
      "article_number: 5223541\n",
      "article_number: 5223741\n",
      "article_number: 5224041\n",
      "article_number: 5224141\n",
      "article_number: 5224241\n",
      "article_number: 5224341\n",
      "article_number: 5224441\n",
      "article_number: 5224741\n",
      "article_number: 5224841\n",
      "article_number: 5225041\n",
      "article_number: 5225141\n",
      "article_number: 5225241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_number: 5225341\n",
      "article_number: 5225441\n",
      "article_number: 5225841\n",
      "article_number: 5226041\n",
      "article_number: 5226141\n",
      "article_number: 5226241\n",
      "article_number: 5226341\n",
      "article_number: 5226441\n",
      "article_number: 5226541\n",
      "article_number: 5226641\n",
      "article_number: 5226741\n",
      "article_number: 5227241\n",
      "article_number: 5227341\n",
      "article_number: 5227441\n",
      "article_number: 5227541\n",
      "article_number: 5227941\n",
      "article_number: 5228041\n",
      "article_number: 5228141\n",
      "article_number: 5228241\n",
      "article_number: 5229041\n",
      "article_number: 5229141\n",
      "article_number: 5229241\n",
      "article_number: 5229441\n",
      "article_number: 5229641\n",
      "article_number: 5230041\n",
      "article_number: 5230141\n",
      "article_number: 5230341\n",
      "article_number: 5230441\n",
      "article_number: 5231141\n",
      "article_number: 5231241\n",
      "article_number: 5231341\n",
      "article_number: 5231541\n",
      "article_number: 5231641\n",
      "article_number: 5231741\n",
      "article_number: 5232141\n",
      "article_number: 5232241\n",
      "article_number: 5232341\n",
      "article_number: 5232541\n",
      "article_number: 5232641\n",
      "article_number: 5233041\n",
      "article_number: 5233141\n",
      "article_number: 5233241\n",
      "article_number: 5233341\n",
      "article_number: 5234041\n",
      "article_number: 5234141\n",
      "article_number: 5234241\n",
      "article_number: 5234341\n",
      "article_number: 5234441\n",
      "article_number: 5234941\n",
      "article_number: 5235041\n",
      "article_number: 5235141\n",
      "article_number: 5235241\n",
      "article_number: 5235341\n",
      "article_number: 5235441\n",
      "article_number: 5236141\n",
      "article_number: 5236241\n",
      "article_number: 5236341\n",
      "article_number: 5236441\n",
      "article_number: 5236841\n",
      "article_number: 5237041\n",
      "article_number: 5237141\n",
      "article_number: 5237241\n",
      "article_number: 5237341\n",
      "article_number: 5237541\n",
      "article_number: 5238041\n",
      "article_number: 5238141\n",
      "article_number: 5238241\n",
      "article_number: 5238341\n",
      "article_number: 5239141\n",
      "article_number: 5239241\n",
      "article_number: 5239441\n",
      "article_number: 5239541\n",
      "article_number: 5240041\n",
      "article_number: 5240141\n",
      "article_number: 5240241\n",
      "article_number: 5240341\n",
      "article_number: 5240441\n",
      "article_number: 5240941\n",
      "article_number: 5241041\n",
      "article_number: 5241141\n",
      "article_number: 5241341\n",
      "article_number: 5241441\n",
      "article_number: 5241541\n",
      "article_number: 5241641\n",
      "article_number: 5241841\n",
      "article_number: 5242041\n",
      "article_number: 5242141\n",
      "article_number: 5242241\n",
      "article_number: 5242341\n",
      "article_number: 5242441\n",
      "article_number: 5242741\n",
      "article_number: 5242941\n",
      "article_number: 5243141\n",
      "article_number: 5243241\n",
      "article_number: 5243341\n",
      "article_number: 5244041\n",
      "article_number: 5244141\n",
      "article_number: 5244341\n",
      "article_number: 5244441\n",
      "article_number: 5244741\n",
      "article_number: 5244841\n",
      "article_number: 5245041\n",
      "article_number: 5245241\n",
      "article_number: 5246041\n",
      "article_number: 5246141\n",
      "article_number: 5246241\n",
      "article_number: 5246441\n",
      "article_number: 5247041\n",
      "article_number: 5247141\n",
      "article_number: 5247241\n",
      "article_number: 5247341\n",
      "article_number: 5247441\n",
      "article_number: 5248041\n",
      "article_number: 5248141\n",
      "article_number: 5248241\n",
      "article_number: 5248341\n",
      "article_number: 5248441\n",
      "article_number: 5248541\n",
      "article_number: 5249041\n",
      "article_number: 5249241\n",
      "article_number: 5249441\n",
      "article_number: 5259241\n",
      "article_number: 5259441\n",
      "article_number: 5260041\n",
      "article_number: 5260141\n",
      "article_number: 5260241\n",
      "article_number: 5260341\n",
      "article_number: 5260441\n",
      "article_number: 5260541\n",
      "article_number: 5261041\n",
      "article_number: 5261141\n",
      "article_number: 5261241\n",
      "article_number: 5261341\n",
      "article_number: 5261441\n",
      "article_number: 5262041\n",
      "article_number: 5262141\n",
      "article_number: 5262241\n",
      "article_number: 5262341\n",
      "article_number: 5262441\n",
      "article_number: 5263041\n",
      "article_number: 5263141\n",
      "article_number: 5263241\n",
      "article_number: 5263341\n",
      "article_number: 5263441\n",
      "article_number: 5264041\n",
      "article_number: 5264141\n",
      "article_number: 5264241\n",
      "article_number: 5264341\n",
      "article_number: 5264441\n",
      "article_number: 5265041\n",
      "CPU times: user 7min 23s, sys: 47.7 s, total: 8min 10s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_directory = \"data/coca-samples-text/\"\n",
    "# text_files = get_files_text(text_directory)\n",
    "\n",
    "# THIS WAS TAKING TOO LONG SO I AM ONLY DOING SPOK AND TVM\n",
    "# ['text_news.txt', 'text_fic.txt', 'text_web.txt', 'text_spok.txt', 'text_tvm.txt', \n",
    "# 'text_blog.txt', 'text_acad.txt', 'text_mag.txt']\n",
    "text_files = [parse_filename_text(text_directory, 'text_spok.txt'), parse_filename_text(text_directory, 'text_tvm.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(label1, label2): \n",
    "    text_file1 = [file for file in text_files if label1 in file[\"filename\"]][0]\n",
    "    text_file2 = [file for file in text_files if label2 in file[\"filename\"]][0]\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for article in text_file1[\"articles\"]: \n",
    "        X.append(article)\n",
    "        Y.append(label1)\n",
    "    \n",
    "    for article in text_file2[\"articles\"]: \n",
    "        X.append(article)\n",
    "        Y.append(label2)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(\"spok\", \"tvm\")\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    print(f\"Class 1: {clf.classes_[0]}\")\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Class 2: {clf.classes_[1]}\")\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(article):\n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for token in doc: \n",
    "        feat_name = f\"token_{token.text}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in list(doc.sents): \n",
    "        num_tokens = len(list(doc.sents))\n",
    "        feat_name = f\"sentence_length_{num_tokens}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for token in doc: \n",
    "        feat_name = f\"POS_{token.pos_}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['largely', 'generally', 'often', 'rarely', 'sometimes', 'frequently', 'occasionally', 'seldom', 'usually', 'most', 'several', 'some', 'almost', 'practically', 'apparently', 'virtually', 'basically', 'approximately', 'roughly', 'somewhat', 'somehow', 'partially', 'actually', 'like', 'something', 'someone', 'somebody', 'somewhere', 'think', 'thinks', 'thought', 'believe', 'believed', 'believes', 'consider', 'considers', 'considered', 'assume', 'assumes', 'assumed', 'understand', 'understands', 'understood', 'find', 'found', 'finds', 'appear', 'appears', 'appeared', 'seem', 'seems', 'seemed', 'suppose', 'supposes', 'supposed', 'guess', 'guesses', 'guessed', 'estimate', 'estimates', 'estimated', 'speculate', 'speculates', 'speculated', 'suggest', 'suggests', 'suggested', 'may', 'could', 'should', 'might', 'surely', 'probably', 'likely', 'maybe', 'perhaps', 'unsure', 'probable', 'unlikely', 'possibly', 'possible', 'read', 'say', 'says', 'looks like', 'look like', \"don't know\", 'necessarily', 'kind of', 'much', 'bunch', 'couple', 'few', 'little', 'really', 'and all that', 'and so forth', 'et cetera', 'in my mind', 'in my opinion', 'their impression', 'my impression', 'in my understanding', 'my thinking is', 'my understanding is', 'in my view', \"if i'm understanding you correctly\", 'something or other', 'so far', 'at least', 'about', 'around', 'can', 'effectively', 'evidently', 'fairly', 'hopefully', 'in general', 'mainly', 'more or less', 'mostly', 'overall', 'presumably', 'pretty', 'quite clearly', 'quite', 'rather', 'sort of', 'supposedly', 'tend', 'appear to be', 'doubt', 'be sure', 'indicate', 'will', 'must', 'would', 'certainly', 'definitely', 'clearly', 'conceivably', 'certain', 'definite', 'clear', 'assumption', 'possibility', 'probability', 'many', 'almost never', 'improbable', 'always', 'rare', 'consistent with', 'doubtful', 'suggestive', 'diagnostic', 'inconclusive', 'apparent', 'alleged', 'allege', 'a bit', 'presumable']\n"
     ]
    }
   ],
   "source": [
    "hedging_file = open(\"data/hedging_data.txt\")\n",
    "hedging_text = hedging_file.read()\n",
    "hedging_file.close()\n",
    "hedging_lst = [h for h in hedging_text.split(\"\\n\") if (\"%\" not in h and len(h) > 2)]\n",
    "print(hedging_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedging_feature(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in list(doc.sents): \n",
    "        for hedge in hedging_lst: \n",
    "            if hedge in sentence.string: \n",
    "                feat_name = f\"hedge_{hedge}\"\n",
    "                feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [bag_of_words, sentence_length, parts_of_speech, hedging_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.950\n",
      "CPU times: user 14min 1s, sys: 9.51 s, total: 14min 10s\n",
      "Wall time: 14min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf, vocab = pipeline(trainX, testX, trainY, testY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: spok\n",
      "-0.127\ttoken_:\n",
      "-0.059\tsentence_length_241\n",
      "-0.049\tPOS_ADP\n",
      "-0.049\ttoken_--\n",
      "-0.044\tPOS_AUX\n",
      "-0.043\tsentence_length_185\n",
      "-0.040\tsentence_length_105\n",
      "-0.039\ttoken_to\n",
      "-0.038\tsentence_length_181\n",
      "-0.038\ttoken_,\n",
      "-0.037\ttoken_#\n",
      "-0.036\tsentence_length_126\n",
      "-0.035\tsentence_length_64\n",
      "-0.034\tPOS_CCONJ\n",
      "-0.034\tsentence_length_149\n",
      "-0.033\tPOS_PROPN\n",
      "-0.027\tsentence_length_114\n",
      "-0.026\ttoken_-\n",
      "-0.025\tPOS_X\n",
      "-0.025\ttoken_and\n",
      "\n",
      "Class 2: tvm\n",
      "0.105\tsentence_length_935\n",
      "0.099\tsentence_length_66\n",
      "0.079\tsentence_length_98\n",
      "0.067\ttoken_?\n",
      "0.055\ttoken_...\n",
      "0.053\ttoken_!\n",
      "0.047\tPOS_VERB\n",
      "0.045\ttoken_.\n",
      "0.042\tPOS_PRON\n",
      "0.039\tsentence_length_718\n",
      "0.038\tsentence_length_273\n",
      "0.031\ttoken_(\n",
      "0.031\tsentence_length_272\n",
      "0.028\tsentence_length_239\n",
      "0.028\ttoken_I\n",
      "0.026\tsentence_length_340\n",
      "0.024\tsentence_length_701\n",
      "0.023\tsentence_length_559\n",
      "0.023\ttoken_me\n",
      "0.022\tsentence_length_521\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE & LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# save models\n",
    "dt_string = datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")\n",
    "clf_name = f\"models/clf {dt_string}.sav\"\n",
    "vocab_name = f\"models/vocab {dt_string}.sav\"\n",
    "pickle.dump(clf, open(clf_name, 'wb'))\n",
    "pickle.dump(vocab, open(vocab_name, 'wb'))\n",
    "\n",
    "# load models\n",
    "loaded_clf = pickle.load(open(clf_name, 'rb'))\n",
    "loaded_vocab = pickle.load(open(vocab_name, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Litbank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "litbank_dir = \"../litbank/original/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_litbank(text, number=None, filename=None): \n",
    "    doc = nlp(text)\n",
    "    article = {\"doc\" : doc}\n",
    "    if number != None: \n",
    "        article[\"number\"] = number\n",
    "    if filename != None: \n",
    "        article[\"filename\"] = filename\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_litbank_data(): \n",
    "    X = []\n",
    "    number = 0\n",
    "    for filename in os.listdir(litbank_dir):\n",
    "        if not filename.endswith(\".txt\"): \n",
    "            continue\n",
    "        print(f\"filename: {filename}\")\n",
    "        full_filename = os.path.join(litbank_dir, filename)\n",
    "        file = open(full_filename)\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        # X.append(parse_text_litbank(number, text))\n",
    "        \n",
    "        # chunks = [text[i:i + nlp.max_length] for i in range(0, len(text), nlp.max_length)]\n",
    "        # for chunk in chunks: \n",
    "            # X.append(parse_text_litbank(chunk, number=number, filename=filename))\n",
    "            # number += 1\n",
    "        \n",
    "        text = text[:nlp.max_length]\n",
    "        X.append(parse_text_litbank(text, number=number, filename=filename))\n",
    "        number += 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: 730_oliver_twist.txt\n",
      "filename: 76_adventures_of_huckleberry_finn.txt\n",
      "filename: 74_the_adventures_of_tom_sawyer.txt\n",
      "filename: 766_david_copperfield.txt\n",
      "filename: 345_dracula.txt\n",
      "filename: 105_persuasion.txt\n",
      "filename: 18581_adrift_in_new_york_tom_and_florence_braving_the_world.txt\n",
      "filename: 45_anne_of_green_gables.txt\n",
      "filename: 3268_the_mysteries_of_udolpho.txt\n",
      "filename: 6593_history_of_tom_jones_a_foundling.txt\n",
      "filename: 1206_the_flying_u_ranch.txt\n",
      "filename: 969_the_tenant_of_wildfell_hall.txt\n",
      "filename: 5348_ragged_dick_or_street_life_in_new_york_with_the_bootblacks.txt\n",
      "filename: 84_frankenstein_or_the_modern_prometheus.txt\n",
      "filename: 711_allan_quatermain.txt\n",
      "filename: 351_of_human_bondage.txt\n",
      "filename: 215_the_call_of_the_wild.txt\n",
      "filename: 1327_elizabeth_and_her_german_garden.txt\n",
      "filename: 78_tarzan_of_the_apes.txt\n",
      "filename: 60_the_scarlet_pimpernel.txt\n",
      "filename: 36_the_war_of_the_worlds.txt\n",
      "filename: 599_vanity_fair.txt\n",
      "filename: 2852_the_hound_of_the_baskervilles.txt\n",
      "filename: 2775_the_good_soldier.txt\n",
      "filename: 4276_north_and_south.txt\n",
      "filename: 1695_the_man_who_was_thursday_a_nightmare.txt\n",
      "filename: 367_country_of_the_pointed_firs.txt\n",
      "filename: 3457_the_man_of_the_forest.txt\n",
      "filename: 1661_the_adventures_of_sherlock_holmes.txt\n",
      "filename: 113_the_secret_garden.txt\n",
      "filename: 24_o_pioneers.txt\n",
      "filename: 208_daisy_miller_a_study.txt\n",
      "filename: 1342_pride_and_prejudice.txt\n",
      "filename: 1400_great_expectations.txt\n",
      "filename: 219_heart_of_darkness.txt\n",
      "filename: 44_the_song_of_the_lark.txt\n",
      "filename: 472_the_house_behind_the_cedars.txt\n",
      "filename: 4217_a_portrait_of_the_artist_as_a_young_man.txt\n",
      "filename: 209_the_turn_of_the_screw.txt\n",
      "filename: 1155_the_secret_adversary.txt\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_litbank = build_litbank_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Litbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(X): \n",
    "    X_feat = build_features(X, features)\n",
    "    X_ids=features_to_ids(X_feat, vocab)\n",
    "    return (clf.predict(X_ids), clf.predict_proba(X_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(classes, probas) = predict_data(X_litbank)\n",
    "print(f\"classes: {classes}\")\n",
    "print(f\"probas: {probas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE & LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")\n",
    "\n",
    "# save COCA data\n",
    "trainX_name = f\"models/trainX {dt_string}.sav\"\n",
    "pickle.dump(trainX, open(trainX_name, 'wb'))\n",
    "\n",
    "testX_name = f\"models/testX {dt_string}.sav\"\n",
    "pickle.dump(testX, open(testX_name, 'wb'))\n",
    "\n",
    "trainY_name = f\"models/trainY {dt_string}.sav\"\n",
    "pickle.dump(trainY, open(trainY_name, 'wb'))\n",
    "\n",
    "testY_name = f\"models/testY {dt_string}.sav\"\n",
    "pickle.dump(testY, open(testY_name, 'wb'))\n",
    "\n",
    "# save litbank data\n",
    "X_litbank_name = f\"models/X_litbank {dt_string}.sav\"\n",
    "pickle.dump(X_litbank, open(X_litbank_name, 'wb'))\n",
    "\n",
    "# load COCA data\n",
    "loaded_trainX = pickle.load(open(trainX_name, 'rb'))\n",
    "loaded_testX = pickle.load(open(testX_name, 'rb'))\n",
    "loaded_trainY = pickle.load(open(trainY_name, 'rb'))\n",
    "loaded_testY = pickle.load(open(testY_name, 'rb'))\n",
    "\n",
    "#load litbank data\n",
    "loaded_X_litbank = pickle.load(open(X_litbank_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

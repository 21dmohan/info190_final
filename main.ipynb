{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coca-samples-wlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article_WLP: \n",
    "    def __init__(self, number, w, l, p): \n",
    "        self.number = number\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.p = p\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.number : {\"w\" : self.w[:10], \"l\" : self.l[:10], \"p\" : self.p[:10]}}) + \"/n\"\n",
    "\n",
    "class File_WLP: \n",
    "    def __init__(self, filename, articles):\n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wlp(line): \n",
    "    temp = line.split('\\t')\n",
    "    if len(temp) != 4:\n",
    "        return None\n",
    "    return tuple(temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_wlp(text): \n",
    "    lines = text.split(\"\\n\")\n",
    "    number = None\n",
    "    for line in lines: \n",
    "        numbers = re.findall(r'\\d+', line)\n",
    "        if len(numbers) > 0: \n",
    "            number = int(numbers[0])\n",
    "            break\n",
    "    \n",
    "    if number == None: \n",
    "        return None\n",
    "    \n",
    "    w = []\n",
    "    l = []\n",
    "    p = []\n",
    "    for line in lines: \n",
    "        args = get_wlp(line)\n",
    "        if args == None: \n",
    "            continue\n",
    "        w.append(args[0])\n",
    "        l.append(args[1])\n",
    "        p.append(args[2])\n",
    "    \n",
    "    return Article_WLP(number, w, l, p)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename_wlp(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    pattern = r'\\d+\\t@@\\d+\\t\\t'\n",
    "    article_texts = re.split(pattern, file_text)\n",
    "    \n",
    "    articles = []\n",
    "    for text in article_texts: \n",
    "        article = parse_article_wlp(text)\n",
    "        if article == None: \n",
    "            continue\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None    \n",
    "    return File_WLP(filename, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_wlp(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        file = parse_filename_wlp(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coca-samples-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article_Text: \n",
    "    def __init__(self, number, text, sentence_texts): \n",
    "        self.number = number\n",
    "        self.text = text\n",
    "        self.sentence_texts = sentence_texts\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({self.number : {\"text\" : self.text[:50], \"sentences\" : [s[:10] for s in self.sentence_texts[:3]]}})\n",
    "\n",
    "class File_Text: \n",
    "    def __init__(self, filename, articles): \n",
    "        self.filename = filename\n",
    "        self.articles = articles\n",
    "    \n",
    "    def __str__(self): \n",
    "        return str({self.filename : [str(a) for a in self.articles[:3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles_text(file_text): \n",
    "    article_texts = file_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "        if len(article_text) == 0: \n",
    "            continue\n",
    "        \n",
    "        pattern = r'@@\\d+ '\n",
    "        rv = re.findall(pattern, article_text[:20])\n",
    "        if len(rv) == 0: \n",
    "            continue\n",
    "        article_number = int(rv[0][2:-1])\n",
    "\n",
    "        pattern = r\" [\\.|\\?|\\!] \"\n",
    "        sentence_texts = re.split(pattern, article_text)[1:]\n",
    "        if len(sentence_texts) == 0: \n",
    "            continue\n",
    "        \n",
    "        article = Article_Text(article_number, article_text, sentence_texts)\n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename_text(directory, filename): \n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles_text(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = File_Text(filename, articles)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_text(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename_text(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlp_directory = \"data/coca-samples-wlp (1)/\"\n",
    "wlp_files = get_files_wlp(wlp_directory)\n",
    "\n",
    "text_directory = \"data/coca-samples-text/\"\n",
    "text_files = get_files_text(text_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(label1, label2): \n",
    "    wlp_file1 = [file for file in wlp_files if label1 in file.filename][0]\n",
    "    wlp_file2 = [file for file in wlp_files if label2 in file.filename][0]\n",
    "    \n",
    "    text_file1 = [file for file in text_files if label1 in file.filename][0]\n",
    "    text_file2 = [file for file in text_files if label2 in file.filename][0]\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for wlp_a in wlp_file1.articles: \n",
    "        text_a = [a for a in text_file1.articles if a.number == wlp_a.number]\n",
    "        if len(text_a) == 0: \n",
    "            continue\n",
    "        else: \n",
    "            text_a = text_a[0]\n",
    "        X.append((wlp_a, text_a))\n",
    "        Y.append(label1)\n",
    "    \n",
    "    for wlp_a in wlp_file2.articles: \n",
    "        text_a = [a for a in text_file2.articles if a.number == wlp_a.number]\n",
    "        if len(text_a) == 0: \n",
    "            continue\n",
    "        else: \n",
    "            text_a = text_a[0]\n",
    "        X.append((wlp_a, text_a))\n",
    "        Y.append(label2)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(\"spok\", \"tvm\")\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    print(f\"Class 1: {clf.classes_[0]}\")\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Class 2: {clf.classes_[1]}\")\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(article_pair):\n",
    "    article_wlp = article_pair[0]\n",
    "    article_text = article_pair[1]\n",
    "    \n",
    "    feats = {}\n",
    "    for token in article_wlp.l: \n",
    "        feat_name = f\"token_{token}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(article_pair): \n",
    "    article_wlp = article_pair[0]\n",
    "    article_text = article_pair[1]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in article_text.sentence_texts: \n",
    "        num_tokens = len(sentence.split(\" \"))\n",
    "        feat_name = f\"sentence_length_{num_tokens}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech(article_pair): \n",
    "    article_wlp = article_pair[0]\n",
    "    article_text = article_pair[1]\n",
    "    \n",
    "    feats = {}\n",
    "    for pos in article_wlp.p: \n",
    "        feat_name = f\"POS_{pos}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['largely', 'generally', 'often', 'rarely', 'sometimes', 'frequently', 'occasionally', 'seldom', 'usually', 'most', 'several', 'some', 'almost', 'practically', 'apparently', 'virtually', 'basically', 'approximately', 'roughly', 'somewhat', 'somehow', 'partially', 'actually', 'like', 'something', 'someone', 'somebody', 'somewhere', 'think', 'thinks', 'thought', 'believe', 'believed', 'believes', 'consider', 'considers', 'considered', 'assume', 'assumes', 'assumed', 'understand', 'understands', 'understood', 'find', 'found', 'finds', 'appear', 'appears', 'appeared', 'seem', 'seems', 'seemed', 'suppose', 'supposes', 'supposed', 'guess', 'guesses', 'guessed', 'estimate', 'estimates', 'estimated', 'speculate', 'speculates', 'speculated', 'suggest', 'suggests', 'suggested', 'may', 'could', 'should', 'might', 'surely', 'probably', 'likely', 'maybe', 'perhaps', 'unsure', 'probable', 'unlikely', 'possibly', 'possible', 'read', 'say', 'says', 'looks like', 'look like', \"don't know\", 'necessarily', 'kind of', 'much', 'bunch', 'couple', 'few', 'little', 'really', 'and all that', 'and so forth', 'et cetera', 'in my mind', 'in my opinion', 'their impression', 'my impression', 'in my understanding', 'my thinking is', 'my understanding is', 'in my view', \"if i'm understanding you correctly\", 'something or other', 'so far', 'at least', 'about', 'around', 'can', 'effectively', 'evidently', 'fairly', 'hopefully', 'in general', 'mainly', 'more or less', 'mostly', 'overall', 'presumably', 'pretty', 'quite clearly', 'quite', 'rather', 'sort of', 'supposedly', 'tend', 'appear to be', 'doubt', 'be sure', 'indicate', 'will', 'must', 'would', 'certainly', 'definitely', 'clearly', 'conceivably', 'certain', 'definite', 'clear', 'assumption', 'possibility', 'probability', 'many', 'almost never', 'improbable', 'always', 'rare', 'consistent with', 'doubtful', 'suggestive', 'diagnostic', 'inconclusive', 'apparent', 'alleged', 'allege', 'a bit', 'presumable']\n"
     ]
    }
   ],
   "source": [
    "hedging_file = open(\"data/hedging_data.txt\")\n",
    "hedging_text = hedging_file.read()\n",
    "hedging_file.close()\n",
    "hedging_lst = [h for h in hedging_text.split(\"\\n\") if (\"%\" not in h and len(h) > 2)]\n",
    "print(hedging_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedging_feature(article_pair): \n",
    "    article_wlp = article_pair[0]\n",
    "    article_text = article_pair[1]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in article_text.sentence_texts: \n",
    "        for hedge in hedging_lst: \n",
    "            if hedge in sentence: \n",
    "                feat_name = f\"hedge_{hedge}\"\n",
    "                feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [bag_of_words, sentence_length, parts_of_speech, hedging_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "clf, vocab = pipeline(trainX, testX, trainY, testY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: spok\n",
      "-0.133\tPOS_FO\n",
      "-0.099\ttoken_:\n",
      "-0.098\tPOS_fo\n",
      "-0.091\tPOS_:\n",
      "-0.070\tPOS_y\n",
      "-0.061\ttoken_,\n",
      "-0.049\tPOS_vbz\n",
      "-0.042\ttoken_a\n",
      "-0.038\ttoken_to\n",
      "-0.035\tPOS_,\n",
      "-0.033\tPOS_ii\n",
      "-0.032\tPOS_at1\n",
      "-0.030\tPOS_jj\n",
      "-0.029\tPOS_vv0\n",
      "-0.027\tPOS_nn1\n",
      "-0.027\ttoken_so\n",
      "-0.026\ttoken_on\n",
      "-0.026\ttoken_yale\n",
      "-0.025\ttoken_this\n",
      "-0.024\ttoken_his\n",
      "\n",
      "Class 2: tvm\n",
      "0.104\tPOS_@1\n",
      "0.070\tPOS_vvi\n",
      "0.070\tPOS_ppis1\n",
      "0.069\tsentence_length_8\n",
      "0.068\tsentence_length_3\n",
      "0.067\tsentence_length_1\n",
      "0.061\tsentence_length_2\n",
      "0.057\tPOS_!\n",
      "0.057\ttoken_!\n",
      "0.057\tsentence_length_9\n",
      "0.057\ttoken_i\n",
      "0.053\tPOS_?\n",
      "0.051\tsentence_length_6\n",
      "0.050\tPOS_vvd\n",
      "0.048\ttoken_?\n",
      "0.048\tPOS_\"\n",
      "0.046\ttoken_\"\n",
      "0.040\ttoken_...\n",
      "0.040\tPOS_...\n",
      "0.040\tPOS_ppio1\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test to Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "guten_filename = \"../comphumF20/data/fiction.6M.txt\"\n",
    "file = open(guten_filename)\n",
    "guten_text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_guten_data(text): \n",
    "    X = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    for i in range(len(lines)): \n",
    "        line = lines[i]\n",
    "\n",
    "        # Article_Text\n",
    "        sentences = nltk.tokenize.sent_tokenize(line)\n",
    "        a_text = Article_Text(i, line, sentences)\n",
    "\n",
    "        # Article_WLP\n",
    "        lexicon = nltk.word_tokenize(line)\n",
    "        pairs = nltk.pos_tag(lexicon)\n",
    "        l = [pair[0] for pair in pairs]\n",
    "        p = [pair[1] for pair in pairs]\n",
    "        a_wlp = Article_WLP(i, [], l, p)\n",
    "        \n",
    "        X.append((a_wlp, a_text))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 1e+03 ns, total: 10 µs\n",
      "Wall time: 17.6 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "guten_X = build_guten_data(guten_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Article_WLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6eeda7eb73af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/chF20/lib/python3.6/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chF20/lib/python3.6/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chF20/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chF20/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Article_WLP'"
     ]
    }
   ],
   "source": [
    "clf.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

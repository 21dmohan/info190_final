{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orality vs. Literacy in Dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the relationship between patterns of oral language and patterns of written language, creating a classifier with contemporary sources and applying it to corpora from other time periods as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import sklearn\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator\n",
    "import spacy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Parsing COCA here\n",
    "##NER for name detection and stopword prevention (src: Mauro Di Pietro)\n",
    "## predict wit NER\n",
    "txt = dtf[\"text\"].iloc[0]\n",
    "entities = ner(txt).ents\n",
    "## tag text\n",
    "tagged_txt = txt\n",
    "for tag in entities:\n",
    "    tagged_txt = re.sub(tag.text, \"_\".join(tag.text.split()), tagged_txt) \n",
    "## show result\n",
    "print(tagged_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Word Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## src: Mauro Di Pietro\n",
    "## need lst_words\n",
    "## count\n",
    "lst_grams = [len(word.split(\" \")) for word in lst_words]\n",
    "vectorizer = feature_extraction.text.CountVectorizer(\n",
    "                 vocabulary=lst_words, \n",
    "                 ngram_range=(min(lst_grams),max(lst_grams)))\n",
    "dtf_X = pd.DataFrame(vectorizer.fit_transform(dtf[\"text_clean\"]).todense(), columns=lst_words)\n",
    "## add the new features as columns\n",
    "dtf = pd.concat([dtf, dtf_X.set_index(dtf.index)], axis=1)\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Topic Modeling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To use this with a train/test CV split, we have to do a bit more work. see below: \n",
    "## https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, devX, trainY, devY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is not a cleaned version of the pipeline, cut from Classification.ipynb\n",
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is not a cleaned version of the pipeline, cut from Classification.ipynb\n",
    "features=[your_awesome_feature]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coca-samples-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles_text(file_text): \n",
    "    article_texts = file_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    for article_text in article_texts: \n",
    "        if len(article_text) == 0: \n",
    "            continue\n",
    "        \n",
    "        pattern = r'@@\\d+ '\n",
    "        rv = re.findall(pattern, article_text[:20])\n",
    "        if len(rv) == 0: \n",
    "            continue\n",
    "        article_number = int(rv[0][2:-1])\n",
    "        print(f\"article_number: {article_number}\")\n",
    "        article_doc = nlp(article_text)\n",
    "        article = {\"number\" : article_number, \"doc\" : article_doc}\n",
    "        \n",
    "        articles.append(article)\n",
    "    \n",
    "    if len(articles) == 0: \n",
    "        return None\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename_text(directory, filename): \n",
    "    print(f\"filename: {filename}\")\n",
    "    file = open(directory + filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "    file_text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    articles = parse_articles_text(file_text)\n",
    "    if articles == None: \n",
    "        return None\n",
    "    file = {\"filename\" : filename, \"articles\" : articles}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_text(directory): \n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file = parse_filename_text(directory, filename)\n",
    "        if file == None: \n",
    "            continue\n",
    "        files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: text_spok.txt\n",
      "article_number: 17141\n",
      "article_number: 21741\n",
      "article_number: 207541\n",
      "article_number: 207641\n",
      "article_number: 220641\n",
      "article_number: 220741\n",
      "article_number: 221141\n",
      "article_number: 221241\n",
      "article_number: 221341\n",
      "article_number: 221441\n",
      "article_number: 221541\n",
      "article_number: 221641\n",
      "article_number: 221741\n",
      "article_number: 222141\n",
      "article_number: 222241\n",
      "article_number: 222341\n",
      "article_number: 222441\n",
      "article_number: 222541\n",
      "article_number: 222741\n",
      "article_number: 222841\n",
      "article_number: 222941\n",
      "article_number: 223041\n",
      "article_number: 223141\n",
      "article_number: 223241\n",
      "article_number: 223341\n",
      "article_number: 223441\n",
      "article_number: 223541\n",
      "article_number: 223941\n",
      "article_number: 224241\n",
      "article_number: 224341\n",
      "article_number: 224541\n",
      "article_number: 224641\n",
      "article_number: 224741\n",
      "article_number: 224841\n",
      "article_number: 224941\n",
      "article_number: 225041\n",
      "article_number: 225141\n",
      "article_number: 225241\n",
      "article_number: 225341\n",
      "article_number: 225441\n",
      "article_number: 225541\n",
      "article_number: 225641\n",
      "article_number: 225741\n",
      "article_number: 226741\n",
      "article_number: 227641\n",
      "article_number: 228041\n",
      "article_number: 228141\n",
      "article_number: 228241\n",
      "article_number: 228341\n",
      "article_number: 228441\n",
      "article_number: 228541\n",
      "article_number: 228641\n",
      "article_number: 228741\n",
      "article_number: 228841\n",
      "article_number: 228941\n",
      "article_number: 229641\n",
      "article_number: 229741\n",
      "article_number: 230641\n",
      "article_number: 230941\n",
      "article_number: 231041\n",
      "article_number: 231141\n",
      "article_number: 231241\n",
      "article_number: 231441\n",
      "article_number: 231541\n",
      "article_number: 231641\n",
      "article_number: 231741\n",
      "article_number: 231841\n",
      "article_number: 231941\n",
      "article_number: 232041\n",
      "article_number: 232141\n",
      "article_number: 232241\n",
      "article_number: 232441\n",
      "article_number: 232541\n",
      "article_number: 232641\n",
      "article_number: 232741\n",
      "article_number: 232841\n",
      "article_number: 232941\n",
      "article_number: 233041\n",
      "article_number: 233141\n",
      "article_number: 233241\n",
      "article_number: 233641\n",
      "article_number: 234241\n",
      "article_number: 234341\n",
      "article_number: 234441\n",
      "article_number: 234541\n",
      "article_number: 234641\n",
      "article_number: 234741\n",
      "article_number: 234841\n",
      "article_number: 234941\n",
      "article_number: 235041\n",
      "article_number: 235141\n",
      "article_number: 235241\n",
      "article_number: 235341\n",
      "article_number: 235541\n",
      "article_number: 235641\n",
      "article_number: 235741\n",
      "article_number: 235841\n",
      "article_number: 235941\n",
      "article_number: 236041\n",
      "article_number: 236641\n",
      "article_number: 236841\n",
      "article_number: 236941\n",
      "article_number: 237041\n",
      "article_number: 237141\n",
      "article_number: 237241\n",
      "article_number: 237341\n",
      "article_number: 237441\n",
      "article_number: 237541\n",
      "article_number: 237641\n",
      "article_number: 237741\n",
      "article_number: 237941\n",
      "article_number: 238041\n",
      "article_number: 238241\n",
      "article_number: 238341\n",
      "article_number: 238441\n",
      "article_number: 238541\n",
      "article_number: 238841\n",
      "article_number: 238941\n",
      "article_number: 239041\n",
      "article_number: 239241\n",
      "article_number: 239441\n",
      "article_number: 239641\n",
      "article_number: 241841\n",
      "article_number: 241941\n",
      "article_number: 242041\n",
      "article_number: 242141\n",
      "article_number: 242441\n",
      "article_number: 244741\n",
      "article_number: 244841\n",
      "article_number: 244941\n",
      "article_number: 246241\n",
      "article_number: 246341\n",
      "article_number: 246441\n",
      "article_number: 246541\n",
      "article_number: 246641\n",
      "article_number: 246741\n",
      "article_number: 247241\n",
      "article_number: 247341\n",
      "article_number: 247541\n",
      "article_number: 247641\n",
      "article_number: 247741\n",
      "article_number: 247841\n",
      "article_number: 247941\n",
      "article_number: 248041\n",
      "article_number: 248141\n",
      "article_number: 248641\n",
      "article_number: 248741\n",
      "article_number: 248841\n",
      "article_number: 248941\n",
      "article_number: 249241\n",
      "article_number: 249341\n",
      "article_number: 253841\n",
      "article_number: 253941\n",
      "article_number: 254241\n",
      "article_number: 4022141\n",
      "article_number: 4022241\n",
      "article_number: 4023741\n",
      "article_number: 4025441\n",
      "article_number: 4026341\n",
      "article_number: 4026541\n",
      "article_number: 4026941\n",
      "article_number: 4027141\n",
      "article_number: 4027841\n",
      "article_number: 4028241\n",
      "article_number: 4030041\n",
      "article_number: 4030241\n",
      "article_number: 4030941\n",
      "article_number: 4031041\n",
      "article_number: 4031141\n",
      "article_number: 4031241\n",
      "article_number: 4031441\n",
      "article_number: 4031641\n",
      "article_number: 4032041\n",
      "article_number: 4072041\n",
      "article_number: 4072141\n",
      "article_number: 4072241\n",
      "article_number: 4072341\n",
      "article_number: 4072441\n",
      "article_number: 4072641\n",
      "article_number: 4072741\n",
      "article_number: 4072841\n",
      "article_number: 4072941\n",
      "article_number: 4073041\n",
      "article_number: 4081941\n",
      "article_number: 4082041\n",
      "article_number: 4082141\n",
      "article_number: 4082241\n",
      "article_number: 4082341\n",
      "article_number: 4082441\n",
      "article_number: 4082541\n",
      "article_number: 4082641\n",
      "article_number: 4082741\n",
      "article_number: 4090741\n",
      "article_number: 4090841\n",
      "article_number: 4102741\n",
      "article_number: 4102841\n",
      "article_number: 4102941\n",
      "article_number: 4103141\n",
      "article_number: 4103341\n",
      "article_number: 4103441\n",
      "article_number: 4103541\n",
      "article_number: 4103641\n",
      "article_number: 4103741\n",
      "article_number: 4103841\n",
      "article_number: 4103941\n",
      "article_number: 4104041\n",
      "article_number: 4104141\n",
      "article_number: 4104241\n",
      "article_number: 4122841\n",
      "article_number: 4122941\n",
      "article_number: 4123341\n",
      "article_number: 4123541\n",
      "article_number: 4124141\n",
      "article_number: 4124241\n",
      "article_number: 4124441\n",
      "article_number: 4124541\n",
      "article_number: 4124741\n",
      "article_number: 4171141\n",
      "article_number: 4171241\n",
      "article_number: 4171341\n",
      "article_number: 4171441\n",
      "article_number: 4171541\n",
      "article_number: 4171641\n",
      "article_number: 4171741\n",
      "article_number: 4171841\n",
      "article_number: 4171941\n",
      "article_number: 4172141\n",
      "article_number: 4172241\n",
      "article_number: 4172341\n",
      "article_number: 4172441\n",
      "article_number: 4172541\n",
      "article_number: 4172641\n",
      "article_number: 4172642\n",
      "article_number: 4172841\n",
      "article_number: 4172941\n",
      "article_number: 4173041\n",
      "article_number: 5000041\n",
      "article_number: 5000141\n",
      "article_number: 5000241\n",
      "article_number: 5000341\n",
      "article_number: 5000441\n",
      "article_number: 5000541\n",
      "article_number: 5000641\n",
      "article_number: 5000741\n",
      "article_number: 5000841\n",
      "article_number: 5000941\n",
      "article_number: 5001041\n",
      "article_number: 5001141\n",
      "article_number: 5001241\n",
      "article_number: 5001341\n",
      "article_number: 5001441\n",
      "article_number: 5001541\n",
      "article_number: 5001641\n",
      "article_number: 5001741\n",
      "article_number: 5001841\n",
      "article_number: 5001941\n",
      "article_number: 5002041\n",
      "article_number: 5002141\n",
      "article_number: 5002241\n",
      "article_number: 5002341\n",
      "article_number: 5002441\n",
      "article_number: 5002541\n",
      "article_number: 5002641\n",
      "filename: text_fic.txt\n",
      "article_number: 1000041\n",
      "article_number: 1000141\n",
      "article_number: 1000441\n",
      "article_number: 1000541\n",
      "article_number: 1000641\n",
      "article_number: 1000741\n",
      "article_number: 1000841\n",
      "article_number: 1000941\n",
      "article_number: 1001241\n",
      "article_number: 1001341\n",
      "article_number: 1001441\n",
      "article_number: 1001541\n",
      "article_number: 1001741\n",
      "article_number: 1001841\n",
      "article_number: 1001941\n",
      "article_number: 1002241\n",
      "article_number: 1002441\n",
      "article_number: 1002841\n",
      "article_number: 1003041\n",
      "article_number: 1003141\n",
      "article_number: 1003241\n",
      "article_number: 1003641\n",
      "article_number: 1003741\n",
      "article_number: 1003841\n",
      "article_number: 1003941\n",
      "article_number: 1004041\n",
      "article_number: 1004241\n",
      "article_number: 1004541\n",
      "article_number: 1004641\n",
      "article_number: 1005141\n",
      "article_number: 1005241\n",
      "article_number: 1005341\n",
      "article_number: 1005441\n",
      "article_number: 1005541\n",
      "article_number: 1005641\n",
      "article_number: 1005741\n",
      "article_number: 1005941\n",
      "article_number: 1006141\n",
      "article_number: 1006241\n",
      "article_number: 1006341\n",
      "article_number: 1006441\n",
      "article_number: 1006541\n",
      "article_number: 1006641\n",
      "article_number: 1006841\n",
      "article_number: 1006941\n",
      "article_number: 1007341\n",
      "article_number: 1007441\n",
      "article_number: 1007641\n",
      "article_number: 1007741\n",
      "article_number: 1007941\n",
      "article_number: 1008041\n",
      "article_number: 1008241\n",
      "article_number: 1008341\n",
      "article_number: 1008441\n",
      "article_number: 1008641\n",
      "article_number: 1008741\n",
      "article_number: 1008841\n",
      "article_number: 1008941\n",
      "article_number: 1009041\n",
      "article_number: 1009141\n",
      "article_number: 1009241\n",
      "article_number: 1009441\n",
      "article_number: 1009541\n",
      "article_number: 1009841\n",
      "article_number: 1009941\n",
      "article_number: 1010041\n",
      "article_number: 1010241\n",
      "article_number: 1010441\n",
      "article_number: 1010541\n",
      "article_number: 1010641\n",
      "article_number: 1010741\n",
      "article_number: 1010841\n",
      "article_number: 1010941\n",
      "article_number: 1011141\n",
      "article_number: 1011241\n",
      "article_number: 1011441\n",
      "article_number: 1011641\n",
      "article_number: 1011841\n",
      "article_number: 1011941\n",
      "article_number: 1012041\n",
      "article_number: 1012141\n",
      "article_number: 1012241\n",
      "article_number: 1012341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_number: 1012441\n",
      "article_number: 1012641\n",
      "article_number: 1012741\n",
      "article_number: 1012941\n",
      "article_number: 1013041\n",
      "article_number: 1013241\n",
      "article_number: 1013341\n",
      "article_number: 1013441\n",
      "article_number: 1013541\n",
      "article_number: 1013641\n",
      "article_number: 1013741\n",
      "article_number: 1013941\n",
      "article_number: 1014141\n",
      "article_number: 1014241\n",
      "article_number: 1014441\n",
      "article_number: 1014941\n",
      "article_number: 1015341\n",
      "article_number: 1015441\n",
      "article_number: 1015541\n",
      "article_number: 1015641\n",
      "article_number: 1015741\n",
      "article_number: 1015841\n",
      "article_number: 1016041\n",
      "article_number: 1016141\n",
      "article_number: 1016441\n",
      "article_number: 1016541\n",
      "article_number: 1016641\n",
      "article_number: 1016741\n",
      "article_number: 1016841\n",
      "article_number: 1017241\n",
      "article_number: 1017341\n",
      "article_number: 1017541\n",
      "article_number: 1017841\n",
      "article_number: 1018241\n",
      "article_number: 1018541\n",
      "article_number: 1018641\n",
      "article_number: 1018841\n",
      "article_number: 1018941\n",
      "article_number: 1030341\n",
      "article_number: 1030741\n",
      "article_number: 1030841\n",
      "article_number: 1031141\n",
      "article_number: 1031541\n",
      "article_number: 1031641\n",
      "article_number: 1031741\n",
      "article_number: 1032041\n",
      "article_number: 1032441\n",
      "article_number: 1032541\n",
      "article_number: 1032841\n",
      "article_number: 1033341\n",
      "article_number: 1033641\n",
      "article_number: 1033941\n",
      "article_number: 1034041\n",
      "article_number: 1034141\n",
      "article_number: 1034241\n",
      "article_number: 1034641\n",
      "article_number: 1035441\n",
      "article_number: 1035541\n",
      "article_number: 1035741\n",
      "article_number: 1036341\n",
      "article_number: 1036541\n",
      "article_number: 1036641\n",
      "article_number: 1036941\n",
      "article_number: 1037341\n",
      "article_number: 1037441\n",
      "article_number: 1037641\n",
      "article_number: 1037741\n",
      "article_number: 1037941\n",
      "article_number: 1040141\n",
      "article_number: 1040341\n",
      "article_number: 1050241\n",
      "article_number: 1050341\n",
      "article_number: 4023141\n",
      "article_number: 4023241\n",
      "article_number: 4024241\n",
      "article_number: 4024641\n",
      "article_number: 4024941\n",
      "article_number: 4026041\n",
      "article_number: 4026241\n",
      "article_number: 4027341\n",
      "article_number: 4028841\n",
      "article_number: 4040041\n",
      "article_number: 4040141\n",
      "article_number: 4040241\n",
      "article_number: 4040341\n",
      "article_number: 4040441\n",
      "article_number: 4040541\n",
      "article_number: 4040641\n",
      "article_number: 4040741\n",
      "article_number: 4040841\n",
      "article_number: 4040941\n",
      "article_number: 4041041\n",
      "article_number: 4052241\n",
      "article_number: 4052341\n",
      "article_number: 4052441\n",
      "article_number: 4076041\n",
      "article_number: 4076141\n",
      "article_number: 4076241\n",
      "article_number: 4081541\n",
      "article_number: 4081741\n",
      "article_number: 4081841\n",
      "article_number: 4085341\n",
      "article_number: 4085441\n",
      "article_number: 4089441\n",
      "article_number: 4089541\n",
      "article_number: 4090141\n",
      "article_number: 4090241\n",
      "article_number: 4090341\n",
      "article_number: 4090441\n",
      "article_number: 4120141\n",
      "article_number: 4120241\n",
      "article_number: 4120341\n",
      "article_number: 4120841\n",
      "article_number: 4120941\n",
      "article_number: 4121041\n",
      "article_number: 4121141\n",
      "article_number: 4121241\n",
      "article_number: 4121341\n",
      "article_number: 4121441\n",
      "article_number: 4121541\n",
      "article_number: 4121641\n",
      "article_number: 4121841\n",
      "article_number: 4121941\n",
      "article_number: 4122041\n",
      "article_number: 4160041\n",
      "article_number: 4160141\n",
      "article_number: 4160241\n",
      "article_number: 4160341\n",
      "article_number: 4160541\n",
      "article_number: 4160641\n",
      "article_number: 4160841\n",
      "article_number: 4160941\n",
      "article_number: 4161041\n",
      "article_number: 4161141\n",
      "article_number: 4161241\n",
      "article_number: 4161341\n",
      "article_number: 4161441\n",
      "article_number: 4161541\n",
      "article_number: 4161741\n",
      "article_number: 4161941\n",
      "article_number: 4162241\n",
      "article_number: 4162341\n",
      "article_number: 4162541\n",
      "article_number: 4162641\n",
      "article_number: 4162741\n",
      "article_number: 4162841\n",
      "article_number: 4162941\n",
      "article_number: 4163041\n",
      "article_number: 4163141\n",
      "article_number: 4163241\n",
      "article_number: 4163341\n",
      "article_number: 4163441\n",
      "article_number: 4163641\n",
      "article_number: 4163841\n",
      "article_number: 4163941\n",
      "article_number: 4164241\n",
      "article_number: 4164341\n",
      "article_number: 4164441\n",
      "article_number: 4164541\n",
      "article_number: 4164641\n",
      "article_number: 4165941\n",
      "article_number: 4166041\n",
      "article_number: 4166141\n",
      "article_number: 4166241\n",
      "article_number: 4166541\n",
      "article_number: 4166641\n",
      "article_number: 4166741\n",
      "article_number: 4166841\n",
      "article_number: 4167041\n",
      "article_number: 4199041\n",
      "article_number: 4199141\n",
      "article_number: 5002741\n",
      "article_number: 5002841\n",
      "article_number: 5002941\n",
      "article_number: 5003141\n",
      "article_number: 5003241\n",
      "article_number: 5003341\n",
      "article_number: 5003441\n",
      "article_number: 5003541\n",
      "article_number: 5003641\n",
      "article_number: 5003741\n",
      "article_number: 5003841\n",
      "article_number: 5003941\n",
      "article_number: 5004041\n",
      "article_number: 5004141\n",
      "article_number: 5004241\n",
      "article_number: 5250041\n",
      "article_number: 5250141\n",
      "article_number: 5250241\n",
      "article_number: 5250341\n",
      "article_number: 5250441\n",
      "CPU times: user 7min 50s, sys: 47.9 s, total: 8min 38s\n",
      "Wall time: 8min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_directory = \"data/coca-samples-text/\"\n",
    "label1, label2 = \"spok\", \"fic\"\n",
    "# text_files = get_files_text(text_directory)\n",
    "\n",
    "# THIS WAS TAKING TOO LONG SO I AM ONLY DOING SPOK AND TVM\n",
    "# ['text_news.txt', 'text_fic.txt', 'text_web.txt', 'text_spok.txt', 'text_tvm.txt', 'text_blog.txt', 'text_acad.txt', 'text_mag.txt']\n",
    "text_files = [parse_filename_text(text_directory, f\"text_{label1}.txt\"), parse_filename_text(text_directory, f\"text_{label2}.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(label1, label2): \n",
    "    text_file1 = [file for file in text_files if label1 in file[\"filename\"]][0]\n",
    "    text_file2 = [file for file in text_files if label2 in file[\"filename\"]][0]\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for article in text_file1[\"articles\"]: \n",
    "        X.append(article)\n",
    "        Y.append(label1)\n",
    "    \n",
    "    for article in text_file2[\"articles\"]: \n",
    "        X.append(article)\n",
    "        Y.append(label2)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(label1, label2)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    print(f\"Class 1: {clf.classes_[0]}\")\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Class 2: {clf.classes_[1]}\")\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feats(feats): \n",
    "    total = sum(list(feats.values()))\n",
    "    for (key, value) in feats.items(): \n",
    "        feats[key] = value / total\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in list(doc.sents): \n",
    "        num_tokens = len(list(doc.sents))\n",
    "        feat_name = f\"sentence_length_{num_tokens}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return normalize_feats(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for token in doc: \n",
    "        feat_name = f\"POS_{token.pos_}\"\n",
    "        feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return normalize_feats(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['largely', 'generally', 'often', 'rarely', 'sometimes', 'frequently', 'occasionally', 'seldom', 'usually', 'most', 'several', 'some', 'almost', 'practically', 'apparently', 'virtually', 'basically', 'approximately', 'roughly', 'somewhat', 'somehow', 'partially', 'actually', 'like', 'something', 'someone', 'somebody', 'somewhere', 'think', 'thinks', 'thought', 'believe', 'believed', 'believes', 'consider', 'considers', 'considered', 'assume', 'assumes', 'assumed', 'understand', 'understands', 'understood', 'find', 'found', 'finds', 'appear', 'appears', 'appeared', 'seem', 'seems', 'seemed', 'suppose', 'supposes', 'supposed', 'guess', 'guesses', 'guessed', 'estimate', 'estimates', 'estimated', 'speculate', 'speculates', 'speculated', 'suggest', 'suggests', 'suggested', 'may', 'could', 'should', 'might', 'surely', 'probably', 'likely', 'maybe', 'perhaps', 'unsure', 'probable', 'unlikely', 'possibly', 'possible', 'read', 'say', 'says', 'looks like', 'look like', \"don't know\", 'necessarily', 'kind of', 'much', 'bunch', 'couple', 'few', 'little', 'really', 'and all that', 'and so forth', 'et cetera', 'in my mind', 'in my opinion', 'their impression', 'my impression', 'in my understanding', 'my thinking is', 'my understanding is', 'in my view', \"if i'm understanding you correctly\", 'something or other', 'so far', 'at least', 'about', 'around', 'can', 'effectively', 'evidently', 'fairly', 'hopefully', 'in general', 'mainly', 'more or less', 'mostly', 'overall', 'presumably', 'pretty', 'quite clearly', 'quite', 'rather', 'sort of', 'supposedly', 'tend', 'appear to be', 'doubt', 'be sure', 'indicate', 'will', 'must', 'would', 'certainly', 'definitely', 'clearly', 'conceivably', 'certain', 'definite', 'clear', 'assumption', 'possibility', 'probability', 'many', 'almost never', 'improbable', 'always', 'rare', 'consistent with', 'doubtful', 'suggestive', 'diagnostic', 'inconclusive', 'apparent', 'alleged', 'allege', 'a bit', 'presumable']\n"
     ]
    }
   ],
   "source": [
    "hedging_file = open(\"data/hedging_data.txt\")\n",
    "hedging_text = hedging_file.read()\n",
    "hedging_file.close()\n",
    "hedging_lst = [h for h in hedging_text.split(\"\\n\") if (\"%\" not in h and len(h) > 2)]\n",
    "print(hedging_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedging_feature(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in list(doc.sents): \n",
    "        for hedge in hedging_lst: \n",
    "            if hedge in sentence.string: \n",
    "                feat_name = f\"hedge_{hedge}\"\n",
    "                feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return normalize_feats(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidential_list = [\"Heard\", \"say\", \"They say\", \"It seems\", \"I feel that\", \"It sounds like\", \"From my experience\", \"It is said that\", \n",
    "            \"I see\", \"I hear\", \"I gather\", \"I'm told\", \"They say that\", \"I doubt\", \"Apparently\", \"Seemingly\", \"They had\"\n",
    "            \"probably\", \"think\", \"obviously\", \"saw\", \"must\", \"reportedly\", \"looks like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidential_feature(article): \n",
    "    doc = article[\"doc\"]\n",
    "    \n",
    "    feats = {}\n",
    "    for sentence in list(doc.sents): \n",
    "        for evidential in evidential_list: \n",
    "            if evidential in sentence.string: \n",
    "                feat_name = f\"evidential_{evidential}\"\n",
    "                feats[feat_name] = feats.get(feat_name, 0) + 1\n",
    "    return normalize_feats(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [sentence_length, parts_of_speech, hedging_feature, evidential_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.917\n",
      "CPU times: user 16min 43s, sys: 14.8 s, total: 16min 58s\n",
      "Wall time: 17min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf, vocab = pipeline(trainX, testX, trainY, testY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: fic\n",
      "-9.795\thedge_could\n",
      "-8.952\tPOS_VERB\n",
      "-8.810\thedge_around\n",
      "-8.572\thedge_like\n",
      "-8.154\thedge_thought\n",
      "-7.390\tPOS_X\n",
      "-6.876\thedge_says\n",
      "-5.991\tsentence_length_397\n",
      "-5.852\thedge_would\n",
      "-5.670\tsentence_length_378\n",
      "-5.044\tsentence_length_320\n",
      "-4.937\tPOS_NOUN\n",
      "-4.880\tPOS_DET\n",
      "-4.803\thedge_might\n",
      "-4.781\tsentence_length_808\n",
      "-4.637\thedge_read\n",
      "-4.436\tsentence_length_62\n",
      "-4.416\tsentence_length_239\n",
      "-4.399\tsentence_length_49\n",
      "-4.398\tevidential_I hear\n",
      "\n",
      "Class 2: spok\n",
      "16.321\thedge_can\n",
      "14.874\tPOS_PROPN\n",
      "9.716\tPOS_PUNCT\n",
      "8.990\thedge_think\n",
      "8.164\thedge_about\n",
      "7.628\thedge_really\n",
      "6.101\thedge_many\n",
      "5.924\tPOS_AUX\n",
      "5.905\tevidential_think\n",
      "5.738\tsentence_length_74\n",
      "5.483\thedge_will\n",
      "5.303\tevidential_say\n",
      "5.199\tsentence_length_64\n",
      "4.999\tevidential_obviously\n",
      "4.866\thedge_kind of\n",
      "4.785\tsentence_length_196\n",
      "4.682\thedge_much\n",
      "4.563\tsentence_length_315\n",
      "4.095\tsentence_length_241\n",
      "4.002\tsentence_length_457\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE & LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# save models\n",
    "dt_string = datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")\n",
    "clf_name = f\"models/clf {dt_string}.sav\"\n",
    "vocab_name = f\"models/vocab {dt_string}.sav\"\n",
    "pickle.dump(clf, open(clf_name, 'wb'))\n",
    "pickle.dump(vocab, open(vocab_name, 'wb'))\n",
    "\n",
    "# load models\n",
    "loaded_clf = pickle.load(open(clf_name, 'rb'))\n",
    "loaded_vocab = pickle.load(open(vocab_name, 'rb'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Gutenberg Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Gutenberk Texts from before 1923 from [https://github.com/dbamman/litbank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Metadata from ReadMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/litbank_metadata.txt\")\n",
    "litbank_metadata_text = file.read()\n",
    "file.close()\n",
    "\n",
    "litbank_metadata = []\n",
    "for line in litbank_metadata_text.split(\"\\n\")[2:]: \n",
    "    try: \n",
    "        items = line.split(\"|\")\n",
    "        row = {'Gutenberg ID' : int(items[1]), 'Date' : int(items[2]), 'Author' : items[3], 'Title' : items[4]}\n",
    "        litbank_metadata.append(row)\n",
    "    except: \n",
    "        print(f\"skipping this line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guten_year(guten_id): \n",
    "    for row in litbank_metadata: \n",
    "        if row['Gutenberg ID'] == guten_id: \n",
    "            return row['Date']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(190)\n",
    "\"\"\"\n",
    "    Computational limits do not allow us to run spacy on the entirety of one gutenberg texts. \n",
    "    Even running spacy on text[:nlp.max_length] --> waiting 45 minutes to load in the spacy data, \n",
    "    let alone parsing it for features and classifier\n",
    "    Thus, we have decided to take smaller, random chunks from throughout each of the gutenberg texts\n",
    "\"\"\"\n",
    "NUM_SAMPLES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_litbank(text, number=None, filename=None): \n",
    "    doc = nlp(text)\n",
    "    article = {\"doc\" : doc}\n",
    "    if number != None: \n",
    "        article[\"number\"] = number\n",
    "    if filename != None: \n",
    "        article[\"filename\"] = filename\n",
    "        pattern = r\"\\d+\"\n",
    "        guten_id = re.findall(pattern, filename)\n",
    "        if len(guten_id) > 0: \n",
    "            guten_id = int(guten_id[0])\n",
    "            article[\"id\"] = guten_id\n",
    "            guten_year = get_guten_year(guten_id)\n",
    "            if guten_year != None: \n",
    "                article[\"year\"] = guten_year\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_litbank_data(): \n",
    "    X = []\n",
    "    number = 0\n",
    "    for filename in os.listdir(litbank_dir):\n",
    "        if not filename.endswith(\".txt\"): \n",
    "            continue\n",
    "        print(f\"filename: {filename}\")\n",
    "        full_filename = os.path.join(litbank_dir, filename)\n",
    "        file = open(full_filename)\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        chunk_size = nlp.max_length // 100\n",
    "        chunk_texts = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        chunk_sample = random.sample(chunk_texts, min(NUM_SAMPLES, len(chunk_texts)))\n",
    "        for chunk in chunk_sample: \n",
    "            X.append(parse_text_litbank(chunk, number=number, filename=filename))\n",
    "            number += 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: 730_oliver_twist.txt\n",
      "filename: 76_adventures_of_huckleberry_finn.txt\n",
      "filename: 74_the_adventures_of_tom_sawyer.txt\n",
      "filename: 766_david_copperfield.txt\n",
      "filename: 345_dracula.txt\n",
      "filename: 105_persuasion.txt\n",
      "filename: 18581_adrift_in_new_york_tom_and_florence_braving_the_world.txt\n",
      "filename: 45_anne_of_green_gables.txt\n",
      "filename: 3268_the_mysteries_of_udolpho.txt\n",
      "filename: 6593_history_of_tom_jones_a_foundling.txt\n",
      "filename: 1206_the_flying_u_ranch.txt\n",
      "filename: 969_the_tenant_of_wildfell_hall.txt\n",
      "filename: 5348_ragged_dick_or_street_life_in_new_york_with_the_bootblacks.txt\n",
      "filename: 84_frankenstein_or_the_modern_prometheus.txt\n",
      "filename: 711_allan_quatermain.txt\n",
      "filename: 351_of_human_bondage.txt\n",
      "filename: 215_the_call_of_the_wild.txt\n",
      "filename: 1327_elizabeth_and_her_german_garden.txt\n",
      "filename: 78_tarzan_of_the_apes.txt\n",
      "filename: 60_the_scarlet_pimpernel.txt\n",
      "filename: 36_the_war_of_the_worlds.txt\n",
      "filename: 599_vanity_fair.txt\n",
      "filename: 2852_the_hound_of_the_baskervilles.txt\n",
      "filename: 2775_the_good_soldier.txt\n",
      "filename: 4276_north_and_south.txt\n",
      "filename: 1695_the_man_who_was_thursday_a_nightmare.txt\n",
      "filename: 367_country_of_the_pointed_firs.txt\n",
      "filename: 3457_the_man_of_the_forest.txt\n",
      "filename: 1661_the_adventures_of_sherlock_holmes.txt\n",
      "filename: 113_the_secret_garden.txt\n",
      "filename: 24_o_pioneers.txt\n",
      "filename: 208_daisy_miller_a_study.txt\n",
      "filename: 1342_pride_and_prejudice.txt\n",
      "filename: 1400_great_expectations.txt\n",
      "filename: 219_heart_of_darkness.txt\n",
      "filename: 44_the_song_of_the_lark.txt\n",
      "filename: 472_the_house_behind_the_cedars.txt\n",
      "filename: 4217_a_portrait_of_the_artist_as_a_young_man.txt\n",
      "filename: 209_the_turn_of_the_screw.txt\n",
      "filename: 1155_the_secret_adversary.txt\n",
      "filename: 829_gullivers_travels_into_several_remote_nations_of_the_world.txt\n",
      "filename: 15265_the_quest_of_the_silver_fleece_a_novel.txt\n",
      "filename: 9830_the_beautiful_and_damned.txt\n",
      "filename: 2814_dubliners.txt\n",
      "filename: 4300_ulysses.txt\n",
      "filename: 41286_miss_marjoribanks.txt\n",
      "filename: 145_middlemarch.txt\n",
      "filename: 4051_lady_bridget_in_the_nevernever_land_a_story_of_australian_life.txt\n",
      "filename: 6053_evelina_or_the_history_of_a_young_ladys_entrance_into_the_world.txt\n",
      "filename: 73_the_red_badge_of_courage_an_episode_of_the_american_civil_war.txt\n",
      "filename: 95_the_prisoner_of_zenda.txt\n",
      "filename: 521_the_life_and_adventures_of_robinson_crusoe.txt\n",
      "filename: 805_this_side_of_paradise.txt\n",
      "filename: 171_charlotte_temple.txt\n",
      "filename: 33_the_scarlet_letter.txt\n",
      "filename: 432_the_ambassadors.txt\n",
      "filename: 5230_the_invisible_man_a_grotesque_romance.txt\n",
      "filename: 2807_to_have_and_to_hold.txt\n",
      "filename: 932_the_fall_of_the_house_of_usher.txt\n",
      "filename: 11231_bartleby_the_scrivener_a_story_of_wallstreet.txt\n",
      "filename: 550_silas_marner.txt\n",
      "filename: 233_sister_carrie_a_novel.txt\n",
      "filename: 155_the_moonstone.txt\n",
      "filename: 217_sons_and_lovers.txt\n",
      "filename: 1064_the_masque_of_the_red_death.txt\n",
      "filename: 62_a_princess_of_mars.txt\n",
      "filename: 16357_mary_a_fiction.txt\n",
      "filename: 2489_moby_dick.txt\n",
      "filename: 876_life_in_the_ironmills_or_the_korl_woman.txt\n",
      "filename: 174_the_picture_of_dorian_gray.txt\n",
      "filename: 27_far_from_the_madding_crowd.txt\n",
      "filename: 41_the_legend_of_sleepy_hollow.txt\n",
      "filename: 11_alices_adventures_in_wonderland.txt\n",
      "filename: 2095_clotelle_a_tale_of_the_southern_states.txt\n",
      "filename: 8867_the_magnificent_ambersons.txt\n",
      "filename: 434_the_circular_staircase.txt\n",
      "filename: 2641_a_room_with_a_view.txt\n",
      "filename: 541_the_age_of_innocence.txt\n",
      "filename: 543_main_street.txt\n",
      "filename: 514_little_women.txt\n",
      "filename: 271_black_beauty.txt\n",
      "filename: 2005_piccadilly_jim.txt\n",
      "filename: 12677_personality_plus_some_experiences_of_emma_mcchesney_and_her_son_jock.txt\n",
      "filename: 238_dear_enemy.txt\n",
      "filename: 77_the_house_of_the_seven_gables.txt\n",
      "filename: 1023_bleak_house.txt\n",
      "filename: 2891_howards_end.txt\n",
      "filename: 1245_night_and_day.txt\n",
      "filename: 1260_jane_eyre_an_autobiography.txt\n",
      "filename: 768_wuthering_heights.txt\n",
      "filename: 32_herland.txt\n",
      "filename: 974_the_secret_agent_a_simple_tale.txt\n",
      "filename: 2084_the_way_of_all_flesh.txt\n",
      "filename: 160_the_awakening_and_selected_short_stories.txt\n",
      "filename: 120_treasure_island.txt\n",
      "filename: 158_emma.txt\n",
      "filename: 940_the_last_of_the_mohicans_a_narrative_of_1757.txt\n",
      "filename: 2166_king_solomons_mines.txt\n",
      "filename: 502_desert_gold.txt\n",
      "filename: 110_tess_of_the_durbervilles_a_pure_woman.txt\n",
      "CPU times: user 5min 55s, sys: 1.67 s, total: 5min 56s\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "litbank_dir = \"../litbank/original/\"\n",
    "X_litbank = build_litbank_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in More Gutenberg Texts after 1923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_recent_guten(text, number=None, filename=None): \n",
    "    doc = nlp(text)\n",
    "    article = {\"doc\" : doc}\n",
    "    if number != None: \n",
    "        article[\"number\"] = number\n",
    "    if filename != None: \n",
    "        article[\"filename\"] = filename\n",
    "        pattern = r\"\\d+\"\n",
    "        rv = re.findall(pattern, filename)\n",
    "        if len(rv) == 2: \n",
    "            article[\"id\"] = int(rv[0])\n",
    "            article[\"year\"] = int(rv[1])\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recent_guten_data(): \n",
    "    X = []\n",
    "    number = 0\n",
    "    for filename in os.listdir(recent_guten_dir):\n",
    "        if not filename.endswith(\".txt\"): \n",
    "            continue\n",
    "        print(f\"filename: {filename}\")\n",
    "        full_filename = os.path.join(recent_guten_dir, filename)\n",
    "        file = open(full_filename)\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        \n",
    "        \"\"\"\n",
    "        Computational limits do not allow us to run spacy on the entirety of one gutenberg texts. \n",
    "        Even running spacy on text[:nlp.max_length] --> waiting 45 minutes to load in the spacy data, \n",
    "        let alone parsing it for features and classifier\n",
    "        Thus, we have decided to take smaller, random chunks from throughout each of the gutenberg texts\n",
    "        \"\"\"\n",
    "        chunk_size = nlp.max_length // 100\n",
    "        chunk_texts = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        chunk_sample = random.sample(chunk_texts, min(NUM_SAMPLES, len(chunk_texts)))\n",
    "        for chunk in chunk_sample: \n",
    "            X.append(parse_text_recent_guten(chunk, number=number, filename=filename))\n",
    "            number += 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: 63426-his_chap-1955.txt\n",
      "filename: 63942-techmen_1954.txt\n",
      "filename: 63950-star_ship-1950.txt\n",
      "filename: 62864-huey_long-1963.txt\n",
      "filename: 63956-queen_catacombs-1949.txt\n",
      "filename: 63960-rebel_of_valkyr-1950.txt\n",
      "filename: 61278_too_many_eggs-1962.txt\n",
      "filename: 63990-palimpsest-1951.txt\n",
      "filename: 61430-manners_customs_thrid-1963.txt\n",
      "filename: 63982-conquistadors_come-1951.txt\n",
      "filename: 59288-french_painting-1959.txt\n",
      "filename: 61105-knitting-1970.txt\n",
      "CPU times: user 33.7 s, sys: 45 ms, total: 33.7 s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recent_guten_dir = \"data/recent_guten\"\n",
    "X_recent_guten = build_recent_guten_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put together All Gutenberg Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_guten = X_litbank + X_recent_guten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Litbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(X): \n",
    "    X_feat = build_features(X, features)\n",
    "    X_ids=features_to_ids(X_feat, vocab)\n",
    "    return (clf.predict(X_ids), clf.predict_proba(X_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['fic' 'fic' 'fic' 'spok' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'spok' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'spok' 'fic' 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'spok'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'spok' 'spok' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'fic' 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'spok'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'spok' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'spok' 'spok' 'fic' 'spok'\n",
      " 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok'\n",
      " 'spok' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'spok'\n",
      " 'fic' 'fic' 'fic' 'fic' 'spok' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'spok' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'spok' 'spok' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok'\n",
      " 'fic' 'fic' 'fic' 'spok' 'fic' 'spok' 'spok' 'fic' 'spok' 'spok' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic'\n",
      " 'spok' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok'\n",
      " 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok'\n",
      " 'fic' 'fic' 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'spok'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'spok' 'spok' 'spok' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'spok' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'spok' 'fic' 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'fic' 'fic' 'spok' 'fic' 'spok' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'spok' 'fic' 'fic' 'fic' 'fic' 'spok' 'fic' 'fic' 'fic' 'spok' 'fic'\n",
      " 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic' 'fic'\n",
      " 'fic' 'fic' 'spok']\n",
      "probas: [[0.89989229 0.10010771]\n",
      " [0.69151525 0.30848475]\n",
      " [0.85311832 0.14688168]\n",
      " ...\n",
      " [0.53456225 0.46543775]\n",
      " [0.95969002 0.04030998]\n",
      " [0.0672978  0.9327022 ]]\n",
      "CPU times: user 6min 32s, sys: 661 ms, total: 6min 33s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(classes, probas) = predict_data(X_guten)\n",
    "print(f\"classes: {classes}\")\n",
    "print(f\"probas: {probas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spok: 119\n",
      "Number of fic: 755\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of {label1}: {len([c for c in classes if c == label1])}\")\n",
    "print(f\"Number of {label2}: {len([c for c in classes if c == label2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE & LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")\n",
    "\n",
    "# save COCA data\n",
    "trainX_name = f\"models/trainX {dt_string}.sav\"\n",
    "pickle.dump(trainX, open(trainX_name, 'wb'))\n",
    "\n",
    "testX_name = f\"models/testX {dt_string}.sav\"\n",
    "pickle.dump(testX, open(testX_name, 'wb'))\n",
    "\n",
    "trainY_name = f\"models/trainY {dt_string}.sav\"\n",
    "pickle.dump(trainY, open(trainY_name, 'wb'))\n",
    "\n",
    "testY_name = f\"models/testY {dt_string}.sav\"\n",
    "pickle.dump(testY, open(testY_name, 'wb'))\n",
    "\n",
    "# save litbank data\n",
    "X_litbank_name = f\"models/X_litbank {dt_string}.sav\"\n",
    "pickle.dump(X_litbank, open(X_litbank_name, 'wb'))\n",
    "\n",
    "# load COCA data\n",
    "loaded_trainX = pickle.load(open(trainX_name, 'rb'))\n",
    "loaded_testX = pickle.load(open(testX_name, 'rb'))\n",
    "loaded_trainY = pickle.load(open(trainY_name, 'rb'))\n",
    "loaded_testY = pickle.load(open(testY_name, 'rb'))\n",
    "\n",
    "#load litbank data\n",
    "loaded_X_litbank = pickle.load(open(X_litbank_name, 'rb'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
